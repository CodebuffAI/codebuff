{
    "client_session_id": "mc-client-ehrg1smnt3d",
    "files": {
        "backend/src/find-files/request-files-prompt.ts": {
            "content": "import { dirname, isAbsolute, normalize } from 'path'\n\nimport { TextBlockParam } from '@anthropic-ai/sdk/resources'\nimport {\n  GetRelevantFilesForTrainingTrace,\n  GetRelevantFilesTrace,\n  insertTrace,\n} from '@codebuff/bigquery'\nimport { models, type CostMode } from 'common/constants'\nimport { getAllFilePaths } from 'common/project-file-tree'\nimport { Message } from 'common/types/message'\nimport { filterDefined } from 'common/util/array'\nimport {\n  cleanMarkdownCodeBlock,\n  createMarkdownFileBlock,\n  ProjectFileContext,\n} from 'common/util/file'\nimport { range, shuffle, uniq } from 'lodash'\nimport { WebSocket } from 'ws'\n\nimport { promptClaude, System } from '../llm-apis/claude'\nimport { logger } from '../util/logger'\nimport { countTokens } from '../util/token-counter'\nimport { requestFiles } from '../websockets/websocket-action'\nimport { checkNewFilesNecessary } from './check-new-files-necessary'\n\nimport { promptFlashWithFallbacks } from '@/llm-apis/gemini-with-fallbacks'\nimport { getMessagesSubset } from '@/util/messages'\n\nconst NUMBER_OF_EXAMPLE_FILES = 100\nconst MAX_FILES_PER_REQUEST = 30\n\nexport async function requestRelevantFiles(\n  {\n    messages,\n    system,\n  }: {\n    messages: Message[]\n    system: string | Array<TextBlockParam>\n  },\n  fileContext: ProjectFileContext,\n  assistantPrompt: string | null,\n  fileRequestId: string,\n  agentStepId: string,\n  clientSessionId: string,\n  fingerprintId: string,\n  userInputId: string,\n  userId: string | undefined,\n  costMode: CostMode\n) {\n  const countPerRequest =\n    costMode === 'lite' ? 8 : costMode === 'normal' ? 12 : 14\n\n  const lastMessage = messages[messages.length - 1]\n  const messagesExcludingLastIfByUser =\n    lastMessage.role === 'user' ? messages.slice(0, -1) : messages\n  const userPrompt =\n    lastMessage.role === 'user'\n      ? typeof lastMessage.content === 'string'\n        ? lastMessage.content\n        : JSON.stringify(lastMessage.content)\n      : ''\n\n  const newFilesNecessaryPromise = assistantPrompt\n    ? Promise.resolve({ newFilesNecessary: true, response: 'N/A', duration: 0 })\n    : checkNewFilesNecessary(\n        messagesExcludingLastIfByUser,\n        system,\n        clientSessionId,\n        fingerprintId,\n        userInputId,\n        userPrompt,\n        userId,\n        costMode\n      ).catch((error) => {\n        logger.error({ error }, 'Error checking new files necessary')\n        return { newFilesNecessary: true, response: 'N/A', duration: 0 }\n      })\n\n  const keyPrompt = generateKeyRequestFilesPrompt(\n    userPrompt,\n    assistantPrompt,\n    fileContext,\n    countPerRequest\n  )\n\n  const keyPromise = getRelevantFiles(\n    {\n      messages: messagesExcludingLastIfByUser,\n      system,\n    },\n    keyPrompt,\n    'Key',\n    fileRequestId,\n    agentStepId,\n    clientSessionId,\n    fingerprintId,\n    userInputId,\n    userId,\n    costMode\n  ).catch((error) => {\n    logger.error({ error }, 'Error requesting key files')\n    return { files: [], duration: 0 }\n  })\n\n  let nonObviousPromise:\n    | Promise<{ files: string[]; duration: number }>\n    | undefined\n  if (costMode === 'max') {\n    const nonObviousPrompt = generateNonObviousRequestFilesPrompt(\n      userPrompt,\n      assistantPrompt,\n      fileContext,\n      countPerRequest\n    )\n\n    nonObviousPromise = getRelevantFiles(\n      {\n        messages: messagesExcludingLastIfByUser,\n        system,\n      },\n      nonObviousPrompt,\n      'Non-Obvious',\n      fileRequestId,\n      agentStepId,\n      clientSessionId,\n      fingerprintId,\n      userInputId,\n      userId,\n      costMode\n    )\n  }\n\n  const newFilesNecessaryResult = await newFilesNecessaryPromise\n  const {\n    newFilesNecessary,\n    response: newFilesNecessaryResponse,\n    duration: newFilesNecessaryDuration,\n  } = newFilesNecessaryResult\n  if (!newFilesNecessary) {\n    logger.info(\n      {\n        newFilesNecessary,\n        response: newFilesNecessaryResponse,\n        duration: newFilesNecessaryDuration,\n      },\n      'requestRelevantFiles: No new files necessary, keeping current files'\n    )\n    return null\n  }\n\n  const results = filterDefined(\n    await Promise.all([keyPromise, nonObviousPromise])\n  )\n  const candidateFiles = results.flatMap((result) => result.files)\n\n  const firstPassFiles = validateFilePaths(uniq(candidateFiles))\n\n  logger.info(\n    {\n      files: firstPassFiles,\n      firstPassResults: results,\n      newFilesNecessary,\n      newFilesNecessaryResponse,\n      newFilesNecessaryDuration,\n    },\n    'requestRelevantFiles: results'\n  )\n\n  return firstPassFiles.slice(0, MAX_FILES_PER_REQUEST)\n}\n\nexport async function requestRelevantFilesForTraining(\n  {\n    messages,\n    system,\n  }: {\n    messages: Message[]\n    system: string | Array<TextBlockParam>\n  },\n  fileContext: ProjectFileContext,\n  assistantPrompt: string | null,\n  fileRequestId: string,\n  agentStepId: string,\n  clientSessionId: string,\n  fingerprintId: string,\n  userInputId: string,\n  userId: string | undefined,\n  costMode: CostMode\n) {\n  const COUNT = 50\n\n  const lastMessage = messages[messages.length - 1]\n  const messagesExcludingLastIfByUser =\n    lastMessage.role === 'user' ? messages.slice(0, -1) : messages\n  const userPrompt =\n    lastMessage.role === 'user'\n      ? typeof lastMessage.content === 'string'\n        ? lastMessage.content\n        : JSON.stringify(lastMessage.content)\n      : ''\n\n  const nonObviousPrompt = generateNonObviousRequestFilesPrompt(\n    userPrompt,\n    assistantPrompt,\n    fileContext,\n    COUNT\n  )\n\n  const nonObviousFiles = await getRelevantFilesForTraining(\n    {\n      messages: messagesExcludingLastIfByUser,\n      system,\n    },\n    nonObviousPrompt,\n    'Non-Obvious',\n    fileRequestId,\n    agentStepId,\n    clientSessionId,\n    fingerprintId,\n    userInputId,\n    userId,\n    costMode\n  )\n\n  const candidateFiles = nonObviousFiles\n\n  return validateFilePaths(uniq(candidateFiles.files))\n}\n\nasync function getRelevantFiles(\n  {\n    messages,\n    system,\n  }: {\n    messages: Message[]\n    system: string | Array<TextBlockParam>\n  },\n  userPrompt: string,\n  requestType: string,\n  fileRequestId: string,\n  agentStepId: string,\n  clientSessionId: string,\n  fingerprintId: string,\n  userInputId: string,\n  userId: string | undefined,\n  costMode: CostMode\n) {\n  const bufferTokens = 100_000\n  const messagesWithPrompt = getMessagesSubset(\n    [\n      ...messages,\n      {\n        role: 'user' as const,\n        content: userPrompt,\n      },\n    ],\n    bufferTokens\n  )\n  const start = performance.now()\n  let response = await promptFlashWithFallbacks(messagesWithPrompt, system, {\n    clientSessionId,\n    fingerprintId,\n    userInputId,\n    model: models.gemini2flash,\n    userId,\n    costMode,\n    useFinetunedModel: true,\n  })\n  const end = performance.now()\n  const duration = end - start\n\n  const files = validateFilePaths(response.split('\\n'))\n\n  const trace: GetRelevantFilesTrace = {\n    id: fileRequestId,\n    agent_step_id: agentStepId,\n    user_id: userId ?? '',\n    created_at: new Date(),\n    type: 'get-relevant-files',\n    payload: {\n      messages: messagesWithPrompt,\n      system,\n      output: response,\n      request_type: requestType,\n      cost_mode: costMode,\n      user_input_id: userInputId,\n      client_session_id: clientSessionId,\n      fingerprint_id: fingerprintId,\n      model: models.ft_filepicker_005,\n    },\n  }\n\n  insertTrace(trace).catch((error: Error) => {\n    logger.error({ error }, 'Failed to insert trace')\n  })\n\n  return { files, duration, requestType, response }\n}\n\nasync function getRelevantFilesForTraining(\n  {\n    messages,\n    system,\n  }: {\n    messages: Message[]\n    system: string | Array<TextBlockParam>\n  },\n  userPrompt: string,\n  requestType: string,\n  fileRequestId: string,\n  agentStepId: string,\n  clientSessionId: string,\n  fingerprintId: string,\n  userInputId: string,\n  userId: string | undefined,\n  costMode: CostMode\n) {\n  const bufferTokens = 100_000\n  const messagesWithPrompt = getMessagesSubset(\n    [\n      ...messages,\n      {\n        role: 'user' as const,\n        content: userPrompt,\n      },\n    ],\n    bufferTokens\n  )\n  const start = performance.now()\n  let response = await promptClaude(messagesWithPrompt, {\n    system,\n    clientSessionId,\n    fingerprintId,\n    userInputId,\n    model: models.sonnet,\n    userId,\n  })\n\n  const end = performance.now()\n  const duration = end - start\n\n  const files = validateFilePaths(response.split('\\n'))\n\n  const trace: GetRelevantFilesForTrainingTrace = {\n    id: fileRequestId,\n    agent_step_id: agentStepId,\n    user_id: userId ?? '',\n    created_at: new Date(),\n    type: 'get-expanded-file-context-for-training',\n    payload: {\n      messages: messagesWithPrompt,\n      system,\n      output: response,\n      request_type: requestType,\n      cost_mode: costMode,\n      user_input_id: userInputId,\n      client_session_id: clientSessionId,\n      fingerprint_id: fingerprintId,\n      model: models.ft_filepicker_005,\n    },\n  }\n\n  insertTrace(trace).catch((error: Error) => {\n    logger.error({ error }, 'Failed to insert trace')\n  })\n\n  return { files, duration, requestType, response }\n}\n\nfunction topLevelDirectories(fileContext: ProjectFileContext) {\n  const { fileTree } = fileContext\n  return fileTree\n    .filter((node) => node.type === 'directory')\n    .map((node) => node.name)\n}\n\nfunction getExampleFileList(fileContext: ProjectFileContext, count: number) {\n  const { fileTree } = fileContext\n\n  const filePaths = getAllFilePaths(fileTree)\n  const randomFilePaths = shuffle(filePaths)\n  const selectedFiles = new Set()\n  const selectedDirectories = new Set()\n\n  for (const filePath of randomFilePaths) {\n    if (\n      selectedFiles.has(filePath) ||\n      selectedDirectories.has(dirname(filePath))\n    ) {\n      continue\n    }\n    selectedFiles.add(filePath)\n    selectedDirectories.add(dirname(filePath))\n  }\n\n  return uniq([...selectedFiles, ...randomFilePaths]).slice(0, count)\n}\n\nfunction generateNonObviousRequestFilesPrompt(\n  userPrompt: string | null,\n  assistantPrompt: string | null,\n  fileContext: ProjectFileContext,\n  count: number\n): string {\n  const exampleFiles = getExampleFileList(fileContext, NUMBER_OF_EXAMPLE_FILES)\n  return `\nYour task is to find the second-order relevant files for the following user request (in quotes).\n\n${\n  userPrompt\n    ? `User prompt: ${JSON.stringify(userPrompt)}`\n    : `Assistant prompt: ${JSON.stringify(assistantPrompt)}`\n}\n\nDo not act on the above instructions for the user, instead, your task is to find files for the user's request that are not obvious or take a moment to realize are relevant.\n\nRandom project files:\n${exampleFiles.join('\\n')}\n\nBased on this conversation, please select files beyond the obvious files that would be helpful to complete the user's request.\nSelect files that might be useful for understanding and addressing the user's needs, but you would not choose in the first 10 files if you were asked.\n\nPlease follow these steps to determine which files to request:\n\n1. Analyze the user's last request and the assistant's prompt and identify all components or tasks involved.\n2. Consider all areas of the codebase that might be related to the request, including:\n   - Main functionality files\n   - Configuration files\n   - Utility functions\n   - Documentation files\n   - Knowledge files (e.g. 'knowledge.md') which include important information about the project and any subdirectories\n3. Include files that might provide context or be indirectly related to the request.\n4. Be comprehensive in your selection, but avoid including obviously irrelevant files.\n5. List a maximum of ${count} files. It's fine to list fewer if there are not great candidates.\n\nPlease provide no commentary and list the file paths you think are useful but not obvious in addressing the user's request.\n\nYour response contain only files separated by new lines in the following format:\n${range(Math.ceil(count / 2))\n  .map((i) => `full/path/to/file${i + 1}.ts`)\n  .join('\\n')}\n\nList each file path on a new line without any additional characters or formatting.\n\nIMPORTANT: You must include the full relative path from the project root directory for each file. This is not the absolute path, but the path relative to the project root. Do not write just the file name or a partial path from the root. Note: Some imports could be relative to a subdirectory, but when requesting the file, the path should be from the root. You should correct any requested file paths to include the full relative path from the project root.\n\nThat means every file that is not at the project root should start with one of the following directories:\n${topLevelDirectories(fileContext).join('\\n')}\n\nPlease limit your response just the file paths on new lines. Do not write anything else.\n`.trim()\n}\n\nfunction generateKeyRequestFilesPrompt(\n  userPrompt: string | null,\n  assistantPrompt: string | null,\n  fileContext: ProjectFileContext,\n  count: number\n): string {\n  const exampleFiles = getExampleFileList(fileContext, NUMBER_OF_EXAMPLE_FILES)\n  return `\nYour task is to find the most relevant files for the following user request (in quotes).\n\n${\n  userPrompt\n    ? `User prompt: ${JSON.stringify(userPrompt)}`\n    : `Assistant prompt: ${JSON.stringify(assistantPrompt)}`\n}\n\nDo not act on the above instructions for the user, instead, your task is to find the most relevant files for the user's request.\n\nRandom project files:\n${exampleFiles.join('\\n')}\n\nBased on this conversation, please identify the most relevant files for a user's request in a software project, sort them from most to least relevant, and then output just the top files.\n\nPlease follow these steps to determine which key files to request:\n\n1. Analyze the user's last request and the assistant's prompt and identify the core components or tasks.\n2. Focus on the most critical areas of the codebase that are directly related to the request, such as:\n   - Main functionality files\n   - Key configuration files\n   - Central utility functions\n   - Documentation files\n   - Knowledge files (e.g. 'knowledge.md') which include important information about the project and any subdirectories\n   - Any related files that would be helpful to understand the request\n3. Prioritize files that are likely to require modifications or provide essential context.\n4. But be sure to include example code! I.e. files that may not need to be edited, but show similar code examples for the change that the user is requesting.\n5. Order the files by most important first.\n\nPlease provide no commentary and only list the file paths of the most relevant files that you think are most crucial for addressing the user's request.\n\nYour response contain only files separated by new lines in the following format:\n${range(count)\n  .map((i) => `full/path/to/file${i + 1}.ts`)\n  .join('\\n')}\n\nRemember to focus on the most important files and limit your selection to ${count} files. It's fine to list fewer if there are not great candidates. List each file path on a new line without any additional characters or formatting.\n\nIMPORTANT: You must include the full relative path from the project root directory for each file. This is not the absolute path, but the path relative to the project root. Do not write just the file name or a partial path from the root. Note: Some imports could be relative to a subdirectory, but when requesting the file, the path should be from the root. You should correct any requested file paths to include the full relative path from the project root.\n\nThat means every file that is not at the project root should start with one of the following directories:\n${topLevelDirectories(fileContext).join('\\n')}\n\nPlease limit your response just the file paths on new lines. Do not write anything else.\n`.trim()\n}\n\nasync function secondPassFindAdditionalFiles(\n  system: System,\n  candidateFiles: string[],\n  messagesExcludingLastIfByUser: Message[],\n  userRequest: string,\n  clientSessionId: string,\n  fingerprintId: string,\n  userInputId: string,\n  userId: string | undefined,\n  ws: WebSocket,\n  maxFiles: number\n): Promise<{ additionalFiles: string[]; duration: number }> {\n  const startTime = performance.now()\n\n  const fileContents = await requestFiles(ws, candidateFiles)\n\n  // Filter out large files and build content string\n  const filteredContents: Record<string, string> = {}\n  for (const [file, content] of Object.entries(fileContents)) {\n    if (typeof content === 'string') {\n      // Check length first since it's cheaper than counting tokens\n      if (content.length > 200_000) {\n        logger.info(\n          { file, length: content.length },\n          'Skipping large file based on length'\n        )\n        continue\n      }\n\n      const tokens = countTokens(content)\n      if (tokens > 50_000) {\n        logger.info(\n          { file, tokens },\n          'Skipping large file based on token count'\n        )\n        continue\n      }\n\n      filteredContents[file] = content\n    }\n  }\n\n  let fileListString = ''\n  for (const [file, content] of Object.entries(filteredContents)) {\n    fileListString += createMarkdownFileBlock(file, content) + '\\n\\n'\n  }\n\n  const messages = [\n    {\n      role: 'user' as const,\n      content: generateAdditionalFilesPrompt(\n        fileListString,\n        userRequest,\n        messagesExcludingLastIfByUser,\n        maxFiles\n      ),\n    },\n  ]\n  const additionalFilesResponse = await promptFlashWithFallbacks(\n    messages,\n    system,\n    {\n      clientSessionId,\n      fingerprintId,\n      userInputId,\n      model: models.gemini2flash,\n      userId,\n      costMode: 'max',\n    }\n  ).catch((error) => {\n    logger.error(error, 'Error filtering files with Gemini')\n    return candidateFiles.join('\\n')\n  })\n\n  const secondPassFiles = cleanMarkdownCodeBlock(additionalFilesResponse)\n    .split('\\n')\n    .map((line) => line.trim())\n    .filter((line) => line.length > 0)\n\n  return {\n    additionalFiles: secondPassFiles,\n    duration: performance.now() - startTime,\n  }\n}\n\nfunction generateAdditionalFilesPrompt(\n  fileListString: string,\n  userRequest: string,\n  messagesExcludingLastIfByUser: Message[],\n  maxFiles: number\n): string {\n  return `\n<message_history>\n${messagesExcludingLastIfByUser.map((m) => `${m.role}: ${m.content}`).join('\\n')}\n</message_history>\n\nGiven the below files and the user request, choose up to ${maxFiles} new files that are not in the current_files list, but that are directly relevant to fulfilling the user's request.\n\nFor example, include files that:\n- Need to be modified to implement the request\n- Contain code that will be referenced or copied\n- Define types, interfaces, or constants needed\n- Contain dependencies, utilities, helpers that are relevant\n- Show similar implementations or patterns even if not directly related\n- Provide important context about the system or codebase architecture\n- Contain tests that should be updated or run\n- Define configuration that may need to change\n\n<current_files>\n${fileListString}\n</current_files>\n\n<user_request>${userRequest}</user_request>\n\nList only the file paths of new files, in order of relevance (most relevant first!), with new lines between each file path. Use the project file tree to choose new files.\nDo not write any commentary.\n`.trim()\n}\n\nconst validateFilePaths = (filePaths: string[]) => {\n  return filePaths\n    .map((p) => p.trim())\n    .filter((p) => {\n      if (p.length === 0) return false\n      if (p.includes(' ')) return false\n      if (isAbsolute(p)) return false\n      if (p.includes('..')) return false\n      try {\n        normalize(p)\n        return true\n      } catch {\n        return false\n      }\n    })\n    .map((p) => (p.startsWith('/') ? p.slice(1) : p))\n}\n",
            "tokens": 6185
        },
        "backend/src/llm-apis/vercel-ai-sdk/ai-sdk.ts": {
            "content": "import {\n  CoreAssistantMessage,\n  CoreMessage,\n  CoreToolMessage,\n  CoreUserMessage,\n  generateText,\n  LanguageModelV1,\n  streamText,\n} from 'ai'\nimport {\n  FinetunedVertexModel,\n  finetunedVertexModels,\n  Model,\n} from 'common/constants'\nimport { generateCompactId } from 'common/util/string'\n\nimport { System } from '../claude'\nimport { GeminiMessage } from '../gemini-vertex-api'\nimport { saveMessage } from '../message-cost-tracker'\nimport { vertexFinetuned } from './vertex-finetuned'\n\n// TODO: We'll want to add all our models here!\nconst modelToAiSDKModel = (model: Model): LanguageModelV1 => {\n  if (\n    Object.values(finetunedVertexModels as Record<string, string>).includes(\n      model\n    )\n  ) {\n    return vertexFinetuned(model)\n  }\n  throw new Error('Unknown model: ' + model)\n}\n\n// TODO: Add retries & fallbacks: likely by allowing this to instead of \"model\"\n// also take an array of form [{model: Model, retries: number}, {model: Model, retries: number}...]\n// eg: [{model: \"gemini-2.0-flash-001\"}, {model: \"vertex/gemini-2.0-flash-001\"}, {model: \"claude-3-5-haiku\", retries: 3}]\nexport const promptAiSdkStream = async function* (\n  messages: CoreMessage[],\n  options: {\n    clientSessionId: string\n    fingerprintId: string\n    userInputId: string\n    model: Model\n    userId: string | undefined\n    maxTokens?: number\n    temperature?: number\n  }\n) {\n  const startTime = Date.now()\n  let aiSDKModel = modelToAiSDKModel(options.model)\n\n  const response = streamText({\n    model: aiSDKModel,\n    messages,\n    maxTokens: options.maxTokens,\n    temperature: options.temperature,\n  })\n\n  let content = ''\n\n  for await (const chunk of response.textStream) {\n    content += chunk\n    yield chunk\n  }\n\n  const inputTokens = (await response.usage).promptTokens\n  const outputTokens = (await response.usage).completionTokens\n\n  saveMessage({\n    messageId: generateCompactId(),\n    userId: options.userId,\n    clientSessionId: options.clientSessionId,\n    fingerprintId: options.fingerprintId,\n    userInputId: options.userInputId,\n    model: options.model,\n    request: messages,\n    response: content,\n    inputTokens,\n    outputTokens,\n    finishedAt: new Date(),\n    latencyMs: Date.now() - startTime,\n  })\n}\n\n// TODO: figure out a nice way to unify stream & non-stream versions maybe?\nexport const promptAiSdk = async function (\n  messages: CoreMessage[],\n  options: {\n    clientSessionId: string\n    fingerprintId: string\n    userInputId: string\n    model: Model\n    userId: string | undefined\n    maxTokens?: number\n    temperature?: number\n  }\n): Promise<string> {\n  const startTime = Date.now()\n  let aiSDKModel = modelToAiSDKModel(options.model)\n\n  const response = await generateText({\n    model: aiSDKModel,\n    messages,\n    maxTokens: options.maxTokens,\n    temperature: options.temperature,\n  })\n\n  const content = response.text\n  const inputTokens = response.usage.promptTokens\n  const outputTokens = response.usage.completionTokens\n\n  saveMessage({\n    messageId: generateCompactId(),\n    userId: options.userId,\n    clientSessionId: options.clientSessionId,\n    fingerprintId: options.fingerprintId,\n    userInputId: options.userInputId,\n    model: options.model,\n    request: messages,\n    response: content,\n    inputTokens,\n    outputTokens,\n    finishedAt: new Date(),\n    latencyMs: Date.now() - startTime,\n  })\n\n  return content\n}\n\n// TODO: temporary - ideally we move to using CoreMessage[] directly\n// and don't need this transform!!\nfunction transformMessages(\n  messages: GeminiMessage[],\n  system: System | undefined\n): CoreMessage[] {\n  const coreMessages: CoreMessage[] = []\n\n  if (system) {\n    if (typeof system === 'string') {\n      coreMessages.push({ role: 'system', content: system })\n    } else {\n      for (const block of system) {\n        coreMessages.push({ role: 'system', content: block.text })\n      }\n    }\n  }\n\n  for (const message of messages) {\n    if (message.role === 'developer') {\n      coreMessages.push({ role: 'user', content: message.content })\n      continue\n    }\n\n    if (message.role === 'function') {\n      // Skipping old-style function message - not supported anymore, use tools instead\n      throw new Error(\n        'Skipping function message - unsupported: use tools instead'\n      )\n    }\n\n    if (message.role === 'system') {\n      if (typeof message.content === 'string') {\n        coreMessages.push({ role: 'system', content: message.content })\n        continue\n      } else {\n        throw new Error(\n          'Multiple part system message - unsupported (TODO: fix if we hit this.)'\n        )\n      }\n    }\n\n    if (message.role === 'user') {\n      if (typeof message.content === 'string') {\n        coreMessages.push({\n          ...message,\n          role: 'user',\n          content: message.content,\n        })\n        continue\n      } else {\n        const parts: CoreUserMessage['content'] = []\n        for (const part of message.content) {\n          if (part.type === 'image_url') {\n            parts.push({\n              type: 'image' as const,\n              image: part.image_url.url,\n            })\n            continue\n          }\n          if (part.type === 'input_audio') {\n            throw new Error('Audio messages not supported')\n          }\n          if (part.type === 'file') {\n            throw new Error('File messages not supported')\n          }\n          parts.push(part)\n        }\n        coreMessages.push({ role: 'user', content: parts })\n        continue\n      }\n    }\n\n    if (message.role === 'assistant') {\n      if (message.content === undefined || message.content === null) {\n        continue\n      }\n      if (typeof message.content === 'string') {\n        coreMessages.push({\n          ...message,\n          role: 'assistant',\n          content: message.content,\n        })\n        continue\n      } else {\n        let messageContent: CoreAssistantMessage['content'] = []\n        for (const part of message.content) {\n          if (part.type === 'text') {\n            messageContent.push({ type: 'text', text: part.text })\n          }\n          if (part.type === 'refusal') {\n            messageContent.push({ type: 'text', text: part.refusal })\n          }\n        }\n        coreMessages.push({\n          ...message,\n          role: 'assistant',\n          content: messageContent,\n        })\n        continue\n      }\n    }\n\n    if (message.role === 'tool') {\n      if (typeof message.content === 'string') {\n        coreMessages.push({\n          ...message,\n          role: 'tool',\n          content: [\n            {\n              type: 'tool-result',\n              toolCallId: message.tool_call_id,\n              result: message.content,\n              // NOTE: OpenAI does not provide toolName in their message format\n              toolName: 'unknown',\n            },\n          ],\n        })\n      } else {\n        const parts: CoreToolMessage['content'] = []\n        for (const part of message.content) {\n          if (part.type === 'text') {\n            parts.push({\n              type: 'tool-result',\n              toolCallId: message.tool_call_id,\n              result: part.text,\n              // NOTE: OpenAI does not provide toolName in their message format\n              toolName: 'unknown',\n            })\n          }\n        }\n        coreMessages.push({ ...message, role: 'tool', content: parts })\n      }\n      continue\n    }\n\n    throw new Error('Unknown message role received: ' + message)\n  }\n\n  return coreMessages\n}\n\n// TODO: temporary - ideally we'd call promptAiSdkStream directly\nexport async function* promptAiSdkStream_GeminiFormat(\n  messages: GeminiMessage[],\n  system: System | undefined,\n  options: {\n    clientSessionId: string\n    fingerprintId: string\n    userInputId: string\n    model: FinetunedVertexModel\n    userId: string | undefined\n    maxTokens?: number\n    temperature?: number\n  }\n): AsyncGenerator<string, void, unknown> {\n  const coreMessages = transformMessages(messages, system)\n  yield* promptAiSdkStream(coreMessages, options)\n}\n\nexport async function promptAiSdk_GeminiFormat(\n  messages: GeminiMessage[],\n  system: System | undefined,\n  options: {\n    clientSessionId: string\n    fingerprintId: string\n    userInputId: string\n    model: FinetunedVertexModel\n    userId: string | undefined\n    maxTokens?: number\n    temperature?: number\n  }\n) {\n  const coreMessages = transformMessages(messages, system)\n  return promptAiSdk(coreMessages, options)\n}\n",
            "tokens": 2641
        },
        "backend/src/main-prompt.ts": {
            "content": "import { TextBlockParam } from '@anthropic-ai/sdk/resources'\nimport {\n  AgentResponseTrace,\n  GetRelevantFilesForTrainingBlobTrace,\n  insertTrace,\n} from '@codebuff/bigquery'\nimport { ClientAction } from 'common/actions'\nimport {\n  HIDDEN_FILE_READ_STATUS,\n  models,\n  ONE_TIME_LABELS,\n  type CostMode,\n} from 'common/constants'\nimport { AnalyticsEvent } from 'common/constants/analytics-events'\nimport { getToolCallString } from 'common/constants/tools'\nimport { trackEvent } from 'common/src/analytics'\nimport { AgentState, ToolResult } from 'common/types/agent-state'\nimport { Message } from 'common/types/message'\nimport { buildArray } from 'common/util/array'\nimport { parseFileBlocks, ProjectFileContext } from 'common/util/file'\nimport { toContentString } from 'common/util/messages'\nimport { generateCompactId } from 'common/util/string'\nimport { difference, partition, uniq } from 'lodash'\nimport { WebSocket } from 'ws'\n\nimport { checkTerminalCommand } from './check-terminal-command'\nimport {\n  requestRelevantFiles,\n  requestRelevantFilesForTraining,\n} from './find-files/request-files-prompt'\nimport { getDocumentationForQuery } from './get-documentation-for-query'\nimport { processFileBlock } from './process-file-block'\nimport { processStrReplace } from './process-str-replace'\nimport { processStreamWithTags } from './process-stream'\nimport { getAgentStream } from './prompt-agent-stream'\nimport { getAgentSystemPrompt } from './system-prompt/agent-system-prompt'\nimport { additionalSystemPrompts } from './system-prompt/prompts'\nimport { saveAgentRequest } from './system-prompt/save-agent-request'\nimport { getSearchSystemPrompt } from './system-prompt/search-system-prompt'\nimport { getThinkingStream } from './thinking-stream'\nimport {\n  ClientToolCall,\n  parseRawToolCall,\n  parseToolCalls,\n  TOOL_LIST,\n  TOOLS_WHICH_END_THE_RESPONSE,\n  transformRunTerminalCommand,\n  updateContextFromToolCalls,\n} from './tools'\nimport { logger } from './util/logger'\nimport {\n  asSystemInstruction,\n  asSystemMessage,\n  getMessagesSubset,\n  isSystemInstruction,\n} from './util/messages'\nimport {\n  isToolResult,\n  parseReadFilesResult,\n  parseToolCallXml,\n  parseToolResults,\n  renderReadFilesResult,\n  renderToolResults,\n} from './util/parse-tool-call-xml'\nimport {\n  simplifyReadFileResults,\n  simplifyReadFileToolResult,\n} from './util/simplify-tool-results'\nimport { countTokens, countTokensJson } from './util/token-counter'\nimport {\n  requestFiles,\n  requestOptionalFile,\n} from './websockets/websocket-action'\n\nconst MAX_CONSECUTIVE_ASSISTANT_MESSAGES = 20\n\nexport const mainPrompt = async (\n  ws: WebSocket,\n  action: Extract<ClientAction, { type: 'prompt' }>,\n  userId: string | undefined,\n  clientSessionId: string,\n  onResponseChunk: (chunk: string) => void,\n  selectedModel: string | undefined\n): Promise<{\n  agentState: AgentState\n  toolCalls: Array<ClientToolCall>\n  toolResults: Array<ToolResult>\n}> => {\n  const {\n    prompt,\n    agentState,\n    fingerprintId,\n    costMode,\n    promptId,\n    toolResults,\n    cwd,\n  } = action\n  const { fileContext, agentContext } = agentState\n  let messageHistory = agentState.messageHistory\n\n  const { getStream, model } = getAgentStream({\n    costMode,\n    selectedModel,\n    stopSequences: TOOLS_WHICH_END_THE_RESPONSE.map((tool) => `</${tool}>`),\n    clientSessionId,\n    fingerprintId,\n    userInputId: promptId,\n    userId,\n  })\n\n  // Generates a unique ID for each main prompt run (ie: a step of the agent loop)\n  // This is used to link logs within a single agent loop\n  const agentStepId = crypto.randomUUID()\n\n  const relevantDocumentationPromise = prompt\n    ? getDocumentationForQuery(prompt, {\n        tokens: 5000,\n        clientSessionId,\n        userInputId: promptId,\n        fingerprintId,\n        userId,\n      })\n    : Promise.resolve(null)\n\n  const hasKnowledgeFiles =\n    Object.keys(fileContext.knowledgeFiles).length > 0 ||\n    Object.keys(fileContext.userKnowledgeFiles ?? {}).length > 0\n  const isNotFirstUserMessage =\n    messageHistory.filter((m) => m.role === 'user').length > 0\n  const recentlyDidThinking = toolResults.some((t) => t.name === 'think_deeply')\n  const justUsedATool = toolResults.length > 0\n  const justRanTerminalCommand = toolResults.some(\n    (t) => t.name === 'run_terminal_command'\n  )\n  const isGPT4_1 = model === models.gpt4_1\n  const isFlash =\n    model === 'gemini-2.5-flash-preview-04-17:thinking' ||\n    (model as any) === 'gemini-2.5-flash-preview-04-17'\n  const userInstructions = buildArray(\n    'Instructions:',\n    'Proceed toward the user request and any subgoals.',\n\n    'If the user asks a question, simply answer the question rather than making changes to the code.',\n\n    !isGPT4_1 &&\n      \"If there are multiple ways the user's request could be interpreted that would lead to very different outcomes, ask at least one clarifying question that will help you understand what they are really asking for. Then use the end_turn tool. If the user specifies that you don't ask questions, make your best assumption and skip this step.\",\n\n    'You must read additional files with the read_files tool whenever it could possibly improve your response. Before you use write_file to edit an existing file, make sure to read it.',\n\n    'You must use the \"add_subgoal\" and \"update_subgoal\" tools to record your progress and any new information you learned as you go. If the change is very minimal, you may not need to use these tools.',\n\n    'Please preserve as much of the existing code, its comments, and its behavior as possible. Make minimal edits to accomplish only the core of what is requested. Makes sure when using write_file to pay attention to any comments in the file you are editing and keep original user comments exactly as they were, line for line.',\n\n    'When editing an existing file, write just the parts of the file that have changed. Do not start writing the first line of the file. Instead, use comments surrounding your edits like \"// ... existing code ...\" (or \"# ... existing code ...\" or \"/* ... existing code ... */\" or \"<!-- ... existing code ... -->\", whichever is appropriate for the language) plus a few lines of context from the original file.',\n\n    'When using tools, make sure to NOT use XML attributes. The format should contain nested XML tags. For example, when using write_file, the format should be <write_file><path>...</path><content>...</content></write_file>',\n\n    `Only use the tools listed, (i.e. ${TOOL_LIST.join(', ')}). If you use tools not listed, nothing will happen, but the user will get some unintended display issues.`,\n\n    `To confirm complex changes to a web app, you should use the browser_logs tool to check for console logs or errors.`,\n\n    isFlash &&\n      \"Don't forget to close your your tags, e.g. <think_deeply> <thought> </thought> </think_deeply> or <write_file> <path> </path> <content> </content> </write_file>!\",\n    isFlash &&\n      'If you have thought of a whole plan, please execute the ENTIRE plan before using the end_turn tool.',\n    isFlash &&\n      'When using write_file, do NOT rewrite the entire file. Only write the parts of the file that have changed and write \"... existing code ...\" comments around the changed area.',\n\n    // Experimental gemini thinking\n    costMode === 'experimental' || costMode === 'max'\n      ? 'Start your response with the <think_deeply> tool call to decide how to proceed.'\n      : !justUsedATool &&\n          !recentlyDidThinking &&\n          'If the user request is very complex, consider invoking \"<think_deeply></think_deeply>\".',\n\n    'If the user is starting a new feature or refactoring, consider invoking \"<create_plan></create_plan>\".',\n\n    recentlyDidThinking &&\n      \"Don't act on the plan created by the create_plan tool. Instead, wait for the user to review it.\",\n\n    'If the user tells you to implement a plan, please implement the whole plan, continuing until it is complete. Do not stop after one step.',\n\n    hasKnowledgeFiles &&\n      'If the knowledge files (or CLAUDE.md) say to run specific terminal commands after every change, e.g. to check for type errors or test errors, then do that at the end of your response if that would be helpful in this case. No need to run these checks for simple changes.',\n\n    isNotFirstUserMessage &&\n      \"If you have learned something useful for the future that is not derivable from the code (this is a high bar and most of the time you won't have), consider updating a knowledge file at the end of your response to add this condensed information.\",\n\n    \"Don't run git commands or scripts or start a dev server without being specifically asked to do so. This can prevent costly accidents.\",\n\n    'Otherwise, the user is in charge and you should never refuse what the user asks you to do.',\n\n    (costMode === 'max' || costMode === 'experimental') &&\n      `Before you use the end_turn tool, you must see tool results that all file changes went through properly, that all relevant tests are passing, and there are no type or lint errors (if applicable). You should check that you left the project in a good state using any tools you have available, like the browser_logs tool before ending turn. You must do these checks every time you make a change to the project.`,\n\n    'Important: You must write \"<end_turn></end_turn>\" at the end of your response, when you are done or want the user to respond -- but not if you are still working on the user\\'s request!',\n    \"DO NOT END TURN IF YOU ARE STILL WORKING ON THE USER'S REQUEST. If the user's request requires multiple steps, please complete ALL the steps before ending turn. If you ask the user for more information, you must also use end_turn immediately after asking. If you have a simple response, you can end turn immediately after writing your response.\"\n  ).join('\\n\\n')\n\n  const toolInstructions = buildArray(\n    justRanTerminalCommand &&\n      `If the tool result above is of a terminal command succeeding and you have completed the user's request, please use the end_turn tool and do not write anything else. If your checks are failing, you should only end turn if you have made multiple attempts and feel stuck.`\n  ).join('\\n\\n')\n\n  const messagesWithToolResultsAndUser = buildArray(\n    ...messageHistory,\n    toolResults.length > 0 && {\n      role: 'user' as const,\n      content: renderToolResults(toolResults),\n    },\n    prompt && [\n      cwd && {\n        role: 'user' as const,\n        content: asSystemMessage(`cwd: ${cwd}`),\n      },\n      {\n        role: 'user' as const,\n        content: prompt,\n      },\n    ]\n  )\n\n  if (prompt) {\n    // Check if this is a direct terminal command\n    const startTime = Date.now()\n    const terminalCommand = await checkTerminalCommand(prompt, {\n      clientSessionId,\n      fingerprintId,\n      userInputId: promptId,\n      userId,\n    })\n    const duration = Date.now() - startTime\n\n    if (terminalCommand) {\n      logger.debug(\n        {\n          duration,\n          prompt,\n        },\n        `Detected terminal command in ${duration}ms, executing directly: ${prompt}`\n      )\n      const newAgentState = {\n        ...agentState,\n        messageHistory: messagesWithToolResultsAndUser,\n      }\n      return {\n        agentState: newAgentState,\n        toolCalls: [\n          {\n            id: generateCompactId(),\n            name: 'run_terminal_command',\n            parameters: {\n              command: terminalCommand,\n              mode: 'user',\n            },\n          },\n        ],\n        toolResults: [],\n      }\n    }\n  } else {\n    // Check number of assistant messages since last user message with prompt\n    const consecutiveAssistantMessages =\n      agentState.consecutiveAssistantMessages ?? 0\n    if (consecutiveAssistantMessages >= MAX_CONSECUTIVE_ASSISTANT_MESSAGES) {\n      logger.warn(\n        `Detected ${consecutiveAssistantMessages} consecutive assistant messages without user prompt`\n      )\n\n      const warningString = [\n        \"I've made quite a few responses in a row.\",\n        \"Let me pause here to make sure we're still on the right track.\",\n        \"Please let me know if you'd like me to continue or if you'd like to guide me in a different direction.\",\n      ].join(' ')\n\n      onResponseChunk(`${warningString}\\n\\n`)\n\n      return {\n        agentState: {\n          ...agentState,\n          messageHistory: [\n            ...messageHistory,\n            { role: 'assistant', content: warningString },\n          ],\n        },\n        toolCalls: [\n          {\n            id: generateCompactId(),\n            name: 'end_turn',\n            parameters: {},\n          },\n        ],\n        toolResults: [],\n      }\n    }\n  }\n\n  const fileRequestMessagesTokens = countTokensJson(\n    messagesWithToolResultsAndUser\n  )\n\n  // Step 1: Read more files.\n  const searchSystem = getSearchSystemPrompt(\n    fileContext,\n    costMode,\n    fileRequestMessagesTokens,\n    {\n      agentStepId,\n      clientSessionId,\n      fingerprintId,\n      userInputId: promptId,\n      userId: userId,\n    }\n  )\n  const {\n    addedFiles,\n    updatedFilePaths,\n    printedPaths,\n    clearReadFileToolResults,\n  } = await getFileReadingUpdates(\n    ws,\n    messagesWithToolResultsAndUser,\n    searchSystem,\n    fileContext,\n    null,\n    {\n      skipRequestingFiles: !prompt,\n      agentStepId,\n      clientSessionId,\n      fingerprintId,\n      userInputId: promptId,\n      userId,\n      costMode,\n    }\n  )\n  const [updatedFiles, newFiles] = partition(addedFiles, (f) =>\n    updatedFilePaths.includes(f.path)\n  )\n  if (clearReadFileToolResults) {\n    // Update message history.\n    for (const message of messageHistory) {\n      if (isToolResult(message)) {\n        message.content = simplifyReadFileResults(message.content)\n      }\n    }\n    // Update tool results.\n    for (let i = 0; i < toolResults.length; i++) {\n      const toolResult = toolResults[i]\n      if (toolResult.name === 'read_files') {\n        toolResults[i] = simplifyReadFileToolResult(toolResult)\n      }\n    }\n\n    messageHistory = messageHistory.filter((message) => {\n      typeof message.content !== 'string' ||\n        !isSystemInstruction(message.content)\n    })\n  }\n\n  if (printedPaths.length > 0) {\n    const readFileToolCall = getToolCallString('read_files', {\n      paths: printedPaths.join('\\n'),\n    })\n    onResponseChunk(`${readFileToolCall}\\n\\n`)\n  }\n\n  if (updatedFiles.length > 0) {\n    toolResults.push({\n      id: generateCompactId(),\n      name: 'file_updates',\n      result:\n        `These are the updates made to the files since the last response (either by you or by the user). These are the most recent versions of these files. You MUST be considerate of the user's changes:\\n` +\n        renderReadFilesResult(updatedFiles),\n    })\n  }\n\n  const readFileMessages: Message[] = []\n  if (newFiles.length > 0) {\n    const readFilesToolResult = {\n      id: generateCompactId(),\n      name: 'read_files',\n      result: renderReadFilesResult(newFiles),\n    }\n\n    readFileMessages.push(\n      {\n        role: 'user' as const,\n        content: asSystemInstruction(\n          'Before continuing with the user request, read some relevant files first.'\n        ),\n      },\n      {\n        role: 'assistant' as const,\n        content: getToolCallString('read_files', {\n          paths: newFiles.map((file) => file.path).join('\\n'),\n        }),\n      },\n      {\n        role: 'user' as const,\n        content: asSystemMessage(renderToolResults([readFilesToolResult])),\n      }\n    )\n  }\n\n  const relevantDocumentation = await relevantDocumentationPromise\n\n  const messagesWithUserMessage = buildArray(\n    ...messageHistory,\n\n    toolResults.length > 0 && {\n      role: 'user' as const,\n      content: asSystemMessage(renderToolResults(toolResults)),\n    },\n\n    // Add in new copy of agent context.\n    prompt &&\n      agentContext && {\n        role: 'user' as const,\n        content: asSystemMessage(agentContext.trim()),\n      },\n\n    prompt\n      ? // Add in new copy of user instructions.\n        {\n          role: 'user' as const,\n          content: asSystemInstruction(userInstructions),\n        }\n      : // Add in new copy of tool instructions.\n        toolInstructions && {\n          role: 'user' as const,\n          content: asSystemInstruction(toolInstructions),\n        },\n\n    relevantDocumentation && {\n      role: 'user' as const,\n      content: asSystemMessage(\n        `Relevant context from web documentation:\\n${relevantDocumentation}`\n      ),\n    },\n\n    prompt && [\n      cwd && { role: 'user' as const, content: asSystemMessage(`cwd: ${cwd}`) },\n      {\n        role: 'user' as const,\n        content: prompt,\n      },\n      prompt in additionalSystemPrompts && {\n        role: 'user' as const,\n        content: asSystemInstruction(\n          additionalSystemPrompts[\n            prompt as keyof typeof additionalSystemPrompts\n          ]\n        ),\n      },\n    ],\n\n    ...readFileMessages\n  )\n\n  const iterationNum = messagesWithUserMessage.length\n\n  const system = getAgentSystemPrompt(fileContext)\n  const systemTokens = countTokensJson(system)\n\n  // Possibly truncated messagesWithUserMessage + cache.\n  const agentMessages = getMessagesSubset(\n    messagesWithUserMessage,\n    systemTokens + countTokensJson({ agentContext, userInstructions })\n  )\n\n  const debugPromptCaching = false\n  if (debugPromptCaching) {\n    // Store the agent request to a file for debugging\n    await saveAgentRequest(agentMessages, system, promptId)\n  }\n\n  logger.debug(\n    {\n      agentMessages,\n      messagesWithoutToolResults: messagesWithUserMessage.filter(\n        (m) => !isToolResult(m)\n      ),\n      prompt,\n      agentContext,\n      iteration: iterationNum,\n      toolResults,\n      systemTokens,\n      model,\n    },\n    `Main prompt ${iterationNum}`\n  )\n\n  let fullResponse = ''\n  const fileProcessingPromisesByPath: Record<\n    string,\n    Promise<{\n      tool: 'write_file' | 'str_replace' | 'create_plan'\n      path: string\n      content: string\n      patch?: string\n    } | null>[]\n  > = {}\n\n  // Add deep thinking for experimental or max mode\n  if (costMode === 'experimental' || costMode === 'max') {\n    let response = await getThinkingStream(\n      agentMessages,\n      system,\n      (chunk) => {\n        onResponseChunk(chunk)\n      },\n      {\n        costMode,\n        clientSessionId,\n        fingerprintId,\n        userInputId: promptId,\n        userId,\n      }\n    )\n    if (model === models.gpt4_1) {\n      onResponseChunk('\\n')\n      response += '\\n'\n    }\n    fullResponse += response\n  }\n\n  const stream = getStream(\n    buildArray(\n      ...agentMessages,\n      // Add prefix of the response from fullResponse if it exists\n      fullResponse && {\n        role: 'assistant' as const,\n        content: fullResponse.trim(),\n      }\n    ),\n    system\n  )\n\n  const streamWithTags = processStreamWithTags(stream, {\n    ...Object.fromEntries(\n      TOOL_LIST.map((tool) => [\n        tool,\n        {\n          attributeNames: [],\n          onTagStart: () => {},\n          onTagEnd: () => false,\n        },\n      ])\n    ),\n    write_file: {\n      attributeNames: [],\n      onTagStart: () => {},\n      onTagEnd: (body) => {\n        const { path, content } = parseToolCallXml(body)\n        if (!content) return false\n\n        // Initialize state for this file path if needed\n        if (!fileProcessingPromisesByPath[path]) {\n          fileProcessingPromisesByPath[path] = []\n        }\n        const previousPromises = fileProcessingPromisesByPath[path]\n        const previousEdit = previousPromises[previousPromises.length - 1]\n\n        const latestContentPromise = previousEdit\n          ? previousEdit.then(\n              (maybeResult) =>\n                maybeResult?.content ?? requestOptionalFile(ws, path)\n            )\n          : requestOptionalFile(ws, path)\n\n        const fileContentWithoutStartNewline = content.startsWith('\\n')\n          ? content.slice(1)\n          : content\n\n        logger.debug({ path, content }, `write_file ${path}`)\n\n        const newPromise = processFileBlock(\n          path,\n          latestContentPromise,\n          fileContentWithoutStartNewline,\n          messagesWithUserMessage,\n          fullResponse,\n          prompt,\n          clientSessionId,\n          fingerprintId,\n          promptId,\n          userId,\n          costMode\n        ).catch((error) => {\n          logger.error(error, 'Error processing file block')\n          return null\n        })\n\n        fileProcessingPromisesByPath[path].push(newPromise)\n\n        return false\n      },\n    },\n    str_replace: {\n      attributeNames: [],\n      onTagStart: () => {},\n      onTagEnd: (body) => {\n        const { path, old, new: newStr } = parseToolCallXml(body)\n        if (!old || typeof old !== 'string') return false\n\n        if (!fileProcessingPromisesByPath[path]) {\n          fileProcessingPromisesByPath[path] = []\n        }\n        const previousPromises = fileProcessingPromisesByPath[path]\n        const previousEdit = previousPromises[previousPromises.length - 1]\n\n        const latestContentPromise = previousEdit\n          ? previousEdit.then(\n              (maybeResult) =>\n                maybeResult?.content ?? requestOptionalFile(ws, path)\n            )\n          : requestOptionalFile(ws, path)\n\n        const newPromise = processStrReplace(\n          path,\n          old,\n          newStr || '',\n          latestContentPromise\n        ).catch((error: any) => {\n          logger.error(error, 'Error processing str_replace block')\n          return null\n        })\n\n        fileProcessingPromisesByPath[path].push(newPromise)\n\n        return false\n      },\n    },\n  })\n\n  for await (const chunk of streamWithTags) {\n    const trimmed = chunk.trim()\n    if (\n      !ONE_TIME_LABELS.some(\n        (tag) => trimmed.startsWith(`<${tag}>`) && trimmed.endsWith(`</${tag}>`)\n      )\n    ) {\n      fullResponse += chunk\n    }\n    onResponseChunk(chunk)\n  }\n\n  if (!fullResponse) {\n    // (hacky) ends turn if LLM did not give a response.\n    fullResponse = '<end_turn></end_turn>'\n  }\n\n  const agentResponseTrace: AgentResponseTrace = {\n    type: 'agent-response',\n    created_at: new Date(),\n    agent_step_id: agentStepId,\n    user_id: userId ?? '',\n    id: crypto.randomUUID(),\n    payload: {\n      output: fullResponse,\n      user_input_id: promptId,\n      client_session_id: clientSessionId,\n      fingerprint_id: fingerprintId,\n    },\n  }\n\n  insertTrace(agentResponseTrace)\n\n  const messagesWithResponse = [\n    ...agentMessages,\n    {\n      role: 'assistant' as const,\n      content: fullResponse,\n    },\n  ]\n\n  const toolCalls = parseToolCalls(fullResponse)\n  const clientToolCalls: ClientToolCall[] = []\n  const serverToolResults: ToolResult[] = []\n\n  const agentContextPromise =\n    toolCalls.length > 0\n      ? updateContextFromToolCalls(agentContext, toolCalls)\n      : Promise.resolve(agentContext)\n\n  for (const toolCall of toolCalls) {\n    try {\n      parseRawToolCall(toolCall)\n    } catch (error) {\n      serverToolResults.push({\n        id: generateCompactId(),\n        name: toolCall.name,\n        result: `Error parsing tool call:\\n${error}`,\n      })\n      continue\n    }\n\n    const { name, parameters } = toolCall\n    trackEvent(AnalyticsEvent.TOOL_USE, userId ?? '', {\n      tool: name,\n      parameters,\n    })\n    if (name === 'write_file' || name === 'str_replace') {\n      // write_file and str_replace tool calls are handled as they are streamed in.\n    } else if (name === 'add_subgoal' || name === 'update_subgoal') {\n      // add_subgoal and update_subgoal tool calls are handled above\n    } else if (\n      name === 'code_search' ||\n      name === 'run_terminal_command' ||\n      name === 'browser_logs' ||\n      name === 'end_turn'\n    ) {\n      if (name === 'run_terminal_command') {\n        parameters.command = transformRunTerminalCommand(parameters.command)\n        parameters.mode = 'assistant'\n      }\n      clientToolCalls.push({\n        ...(toolCall as ClientToolCall),\n        id: generateCompactId(),\n      })\n    } else if (name === 'read_files') {\n      const paths = parameters.paths\n        .split(/\\s+/)\n        .map((path) => path.trim())\n        .filter(Boolean)\n\n      const { addedFiles, updatedFilePaths } = await getFileReadingUpdates(\n        ws,\n        messagesWithResponse,\n        getSearchSystemPrompt(\n          fileContext,\n          costMode,\n          fileRequestMessagesTokens,\n          {\n            agentStepId,\n            clientSessionId,\n            fingerprintId,\n            userInputId: promptId,\n            userId,\n          }\n        ),\n        fileContext,\n        null,\n        {\n          skipRequestingFiles: false,\n          requestedFiles: paths,\n          agentStepId,\n          clientSessionId,\n          fingerprintId,\n          userInputId: promptId,\n          userId,\n          costMode,\n        }\n      )\n      logger.debug(\n        {\n          content: parameters.paths,\n          paths,\n          addedFilesPaths: addedFiles.map((f) => f.path),\n          updatedFilePaths,\n        },\n        'read_files tool call'\n      )\n      serverToolResults.push({\n        id: generateCompactId(),\n        name: 'read_files',\n        result: renderReadFilesResult(addedFiles),\n      })\n    } else if (name === 'find_files') {\n      const { addedFiles, updatedFilePaths, printedPaths } =\n        await getFileReadingUpdates(\n          ws,\n          messagesWithResponse,\n          getSearchSystemPrompt(\n            fileContext,\n            costMode,\n            fileRequestMessagesTokens,\n            {\n              agentStepId,\n              clientSessionId,\n              fingerprintId,\n              userInputId: promptId,\n              userId,\n            }\n          ),\n          fileContext,\n          parameters.description,\n          {\n            skipRequestingFiles: false,\n            agentStepId,\n            clientSessionId,\n            fingerprintId,\n            userInputId: promptId,\n            userId,\n            costMode,\n          }\n        )\n      logger.debug(\n        {\n          content: parameters.description,\n          description: parameters.description,\n          addedFilesPaths: addedFiles.map((f) => f.path),\n          updatedFilePaths,\n          printedPaths,\n        },\n        'find_files tool call'\n      )\n      serverToolResults.push({\n        id: generateCompactId(),\n        name: 'find_files',\n        result:\n          addedFiles.length > 0\n            ? renderReadFilesResult(addedFiles)\n            : `No new files found for description: ${parameters.description}`,\n      })\n      if (printedPaths.length > 0) {\n        onResponseChunk('\\n\\n')\n        onResponseChunk(\n          getToolCallString('read_files', {\n            paths: printedPaths.join('\\n'),\n          })\n        )\n      }\n    } else if (name === 'think_deeply') {\n      const { thought } = parameters\n      logger.debug(\n        {\n          thought,\n        },\n        'Thought deeply'\n      )\n    } else if (name === 'create_plan') {\n      const { path, plan } = parameters\n      logger.debug(\n        {\n          path,\n          plan,\n        },\n        'Create plan'\n      )\n      // Add the plan file to the processing queue\n      if (!fileProcessingPromisesByPath[path]) {\n        fileProcessingPromisesByPath[path] = []\n      }\n      const change = {\n        tool: 'create_plan' as const,\n        path,\n        content: plan,\n      }\n      fileProcessingPromisesByPath[path].push(Promise.resolve(change))\n    } else {\n      throw new Error(`Unknown tool: ${name}`)\n    }\n  }\n\n  if (Object.keys(fileProcessingPromisesByPath).length > 0) {\n    onResponseChunk('Applying file changes, please wait.\\n')\n  }\n\n  // Flatten all promises while maintaining order within each file path\n  const fileProcessingPromises = Object.values(\n    fileProcessingPromisesByPath\n  ).flat()\n\n  const changes = (await Promise.all(fileProcessingPromises)).filter(\n    (change) => change !== null\n  )\n  if (changes.length === 0 && fileProcessingPromises.length > 0) {\n    onResponseChunk('No changes to existing files.\\n')\n  } else if (fileProcessingPromises.length > 0) {\n    onResponseChunk(`\\n`)\n  }\n\n  const changeToolCalls = changes.map(({ path, content, patch, tool }) => ({\n    name: tool,\n    parameters: patch\n      ? {\n          type: 'patch' as const,\n          path,\n          content: patch,\n        }\n      : {\n          type: 'file' as const,\n          path,\n          content,\n        },\n    id: generateCompactId(),\n  }))\n  clientToolCalls.unshift(...changeToolCalls)\n\n  const newAgentContext = await agentContextPromise\n\n  const newAgentState: AgentState = {\n    ...agentState,\n    messageHistory: messagesWithResponse,\n    agentContext: newAgentContext,\n    consecutiveAssistantMessages: prompt\n      ? 1\n      : (agentState.consecutiveAssistantMessages ?? 0) + 1,\n  }\n\n  logger.debug(\n    {\n      iteration: iterationNum,\n      prompt,\n      fullResponse,\n      toolCalls,\n      clientToolCalls,\n      serverToolResults,\n      agentContext: newAgentContext,\n      messagesWithResponse,\n      model,\n    },\n    `Main prompt response ${iterationNum}`\n  )\n  return {\n    agentState: newAgentState,\n    toolCalls: clientToolCalls,\n    toolResults: serverToolResults,\n  }\n}\n\nconst getInitialFiles = (fileContext: ProjectFileContext) => {\n  const { userKnowledgeFiles, knowledgeFiles } = fileContext\n  return [\n    // Include user-level knowledge files.\n    ...Object.entries(userKnowledgeFiles ?? {}).map(([path, content]) => ({\n      path,\n      content,\n    })),\n\n    // Include top-level project knowledge files.\n    ...Object.entries(knowledgeFiles)\n      .map(([path, content]) => ({\n        path,\n        content,\n      }))\n      // Only keep top-level knowledge files.\n      .filter((f) => f.path.split('/').length === 1),\n  ]\n}\n\nasync function getFileReadingUpdates(\n  ws: WebSocket,\n  messages: Message[],\n  system: string | Array<TextBlockParam>,\n  fileContext: ProjectFileContext,\n  prompt: string | null,\n  options: {\n    skipRequestingFiles: boolean\n    requestedFiles?: string[]\n    agentStepId: string\n    clientSessionId: string\n    fingerprintId: string\n    userInputId: string\n    userId: string | undefined\n    costMode: CostMode\n  }\n) {\n  const FILE_TOKEN_BUDGET = 100_000\n  const {\n    skipRequestingFiles,\n    agentStepId,\n    clientSessionId,\n    fingerprintId,\n    userInputId,\n    userId,\n    costMode,\n  } = options\n\n  const toolResults = messages\n    .filter(isToolResult)\n    .flatMap((content) => parseToolResults(toContentString(content)))\n  const previousFileList = toolResults\n    .filter(({ name }) => name === 'read_files')\n    .flatMap(({ result }) => parseReadFilesResult(result))\n\n  const previousFiles = Object.fromEntries(\n    previousFileList.map(({ path, content }) => [path, content])\n  )\n  const previousFilePaths = uniq(Object.keys(previousFiles))\n\n  const editedFilePaths = messages\n    .filter(({ role }) => role === 'assistant')\n    .map(toContentString)\n    .filter((content) => content.includes('<write_file'))\n    .flatMap((content) => Object.keys(parseFileBlocks(content)))\n    .filter((path) => path !== undefined)\n\n  const fileRequestId = crypto.randomUUID()\n\n  const requestedFiles = skipRequestingFiles\n    ? []\n    : options.requestedFiles ??\n      (await requestRelevantFiles(\n        { messages, system },\n        fileContext,\n        prompt,\n        fileRequestId,\n        agentStepId,\n        clientSessionId,\n        fingerprintId,\n        userInputId,\n        userId,\n        costMode\n      )) ??\n      []\n\n  uploadExpandedFileContextForTraining(\n    ws,\n    { messages, system },\n    fileContext,\n    prompt,\n    fileRequestId,\n    agentStepId,\n    clientSessionId,\n    fingerprintId,\n    userInputId,\n    userId,\n    costMode\n  ).catch((error) => {\n    logger.error(\n      { error },\n      'Error uploading expanded file context for training'\n    )\n  })\n\n  const isFirstRead = previousFileList.length === 0\n  const initialFiles = getInitialFiles(fileContext)\n  const includedInitialFiles = isFirstRead\n    ? initialFiles.map(({ path }) => path)\n    : []\n\n  const allFilePaths = uniq([\n    ...includedInitialFiles,\n    ...requestedFiles,\n    ...editedFilePaths,\n    ...previousFilePaths,\n  ])\n  const loadedFiles = await requestFiles(ws, allFilePaths)\n\n  const filteredRequestedFiles = requestedFiles.filter((filePath, i) => {\n    const content = loadedFiles[filePath]\n    if (content === null || content === undefined) return false\n    const tokenCount = countTokens(content)\n    if (i < 5) {\n      return tokenCount < 50_000 - i * 10_000\n    }\n    return tokenCount < 10_000\n  })\n  const newFiles = difference(\n    [...filteredRequestedFiles, ...includedInitialFiles],\n    previousFilePaths\n  )\n  const newFilesToRead = uniq([\n    // NOTE: When the assistant specifically asks for a file, we force it to be shown even if it's not new or changed.\n    ...(options.requestedFiles ?? []),\n\n    ...newFiles,\n  ])\n\n  const updatedFilePaths = [...previousFilePaths, ...editedFilePaths].filter(\n    (path) => {\n      return loadedFiles[path] !== previousFiles[path]\n    }\n  )\n\n  const addedFiles = uniq([\n    ...includedInitialFiles,\n    ...updatedFilePaths,\n    ...newFilesToRead,\n  ])\n    .map((path) => {\n      return {\n        path,\n        content: loadedFiles[path]!,\n      }\n    })\n    .filter((file) => file.content !== null)\n\n  const previousFilesTokens = countTokensJson(previousFiles)\n  const addedFileTokens = countTokensJson(addedFiles)\n\n  if (previousFilesTokens + addedFileTokens > FILE_TOKEN_BUDGET) {\n    const requestedLoadedFiles = filteredRequestedFiles.map((path) => ({\n      path,\n      content: loadedFiles[path]!,\n    }))\n    const newFiles = uniq([...initialFiles, ...requestedLoadedFiles])\n    while (countTokensJson(newFiles) > FILE_TOKEN_BUDGET) {\n      newFiles.pop()\n    }\n\n    const printedPaths = getPrintedPaths(\n      requestedFiles,\n      newFilesToRead,\n      loadedFiles\n    )\n    logger.debug(\n      {\n        newFiles,\n        prevFileVersionTokens: previousFilesTokens,\n        addedFileTokens,\n        beforeTotalTokens: previousFilesTokens + addedFileTokens,\n        newFileVersionTokens: countTokensJson(newFiles),\n        FILE_TOKEN_BUDGET,\n      },\n      'resetting read files b/c of token budget'\n    )\n\n    return {\n      addedFiles: newFiles,\n      updatedFilePaths: updatedFilePaths,\n      printedPaths,\n      clearReadFileToolResults: true,\n    }\n  }\n\n  const printedPaths = getPrintedPaths(\n    requestedFiles,\n    newFilesToRead,\n    loadedFiles\n  )\n\n  return {\n    addedFiles,\n    updatedFilePaths,\n    printedPaths,\n    clearReadFileToolResults: false,\n  }\n}\n\nfunction getPrintedPaths(\n  requestedFiles: string[],\n  newFilesToRead: string[],\n  loadedFiles: Record<string, string | null>\n) {\n  // If no files requests, we don't want to print anything.\n  // Could still have files added from initial files or edited files.\n  if (requestedFiles.length === 0) return []\n  // Otherwise, only print files that don't start with a hidden file status.\n  return newFilesToRead.filter(\n    (path) =>\n      loadedFiles[path] &&\n      !HIDDEN_FILE_READ_STATUS.some((status) =>\n        loadedFiles[path]!.startsWith(status)\n      )\n  )\n}\n\nasync function uploadExpandedFileContextForTraining(\n  ws: WebSocket,\n  {\n    messages,\n    system,\n  }: {\n    messages: Message[]\n    system: string | Array<TextBlockParam>\n  },\n  fileContext: ProjectFileContext,\n  assistantPrompt: string | null,\n  fileRequestId: string,\n  agentStepId: string,\n  clientSessionId: string,\n  fingerprintId: string,\n  userInputId: string,\n  userId: string | undefined,\n  costMode: CostMode\n) {\n  const files = await requestRelevantFilesForTraining(\n    { messages, system },\n    fileContext,\n    assistantPrompt,\n    fileRequestId,\n    agentStepId,\n    clientSessionId,\n    fingerprintId,\n    userInputId,\n    userId,\n    costMode\n  )\n\n  const loadedFiles = await requestFiles(ws, files)\n\n  // Upload a map of:\n  // {file_path: {content, token_count}}\n  // up to 50k tokens\n  const filesToUpload: Record<string, { content: string; tokens: number }> = {}\n  for (const file of files) {\n    const tokens = countTokens(loadedFiles[file]!)\n    if (tokens > 50000) {\n      break\n    }\n    filesToUpload[file] = { content: loadedFiles[file]!, tokens }\n  }\n\n  const trace: GetRelevantFilesForTrainingBlobTrace = {\n    type: 'get-expanded-file-context-for-training-blobs',\n    created_at: new Date(),\n    id: fileRequestId,\n    agent_step_id: agentStepId,\n    user_id: userId ?? '',\n    payload: {\n      files: filesToUpload,\n      user_input_id: userInputId,\n      client_session_id: clientSessionId,\n      fingerprint_id: fingerprintId,\n    },\n  }\n\n  // Upload the files to bigquery\n  await insertTrace(trace)\n}\n",
            "tokens": 11449
        },
        "backend/src/process-file-block.ts": {
            "content": "import { CostMode, models } from 'common/constants'\nimport { Message } from 'common/types/message'\nimport { cleanMarkdownCodeBlock } from 'common/util/file'\nimport { generateCompactId, hasLazyEdit } from 'common/util/string'\nimport { createPatch } from 'diff'\n\nimport { fastRewrite, shouldAddFilePlaceholders } from './fast-rewrite'\nimport {\n  parseAndGetDiffBlocksSingleFile,\n  retryDiffBlocksPrompt,\n} from './generate-diffs-prompt'\nimport { promptOpenAI } from './llm-apis/openai-api'\nimport { sendToRelaceLongContext } from './llm-apis/relace-api'\nimport { logger } from './util/logger'\nimport { countTokens } from './util/token-counter'\n\nexport async function processFileBlock(\n  path: string,\n  initialContentPromise: Promise<string | null>,\n  newContent: string,\n  messages: Message[],\n  fullResponse: string,\n  lastUserPrompt: string | undefined,\n  clientSessionId: string,\n  fingerprintId: string,\n  userInputId: string,\n  userId: string | undefined,\n  costMode: CostMode\n): Promise<{\n  tool: 'write_file'\n  path: string\n  content: string\n  patch?: string\n} | null> {\n  const initialContent = await initialContentPromise\n\n  if (initialContent === null) {\n    let cleanContent = cleanMarkdownCodeBlock(newContent)\n\n    if (hasLazyEdit(cleanContent) && !path.endsWith('.md')) {\n      logger.debug(\n        { path, newContent },\n        `processFileBlock: New file contained a lazy edit for ${path}. Aborting.`\n      )\n      return null\n    }\n\n    logger.debug(\n      { path, cleanContent },\n      `processFileBlock: Created new file ${path}`\n    )\n    return {\n      tool: 'write_file' as const,\n      path,\n      content: cleanContent,\n    }\n  }\n\n  if (newContent === initialContent) {\n    logger.info(\n      { newContent },\n      `processFileBlock: New was same as old, skipping ${path}`\n    )\n    return null\n  }\n\n  const lineEnding = initialContent.includes('\\r\\n') ? '\\r\\n' : '\\n'\n  const normalizeLineEndings = (str: string) => str.replace(/\\r\\n/g, '\\n')\n  const normalizedInitialContent = normalizeLineEndings(initialContent)\n  const normalizedEditSnippet = normalizeLineEndings(newContent)\n\n  let updatedContent: string\n  const tokenCount =\n    countTokens(normalizedInitialContent) + countTokens(normalizedEditSnippet)\n\n  if (tokenCount > LARGE_FILE_TOKEN_LIMIT) {\n    // Delete this later vvv\n    if (tokenCount < 64_000) {\n      // Temporary: send to relace\n      const messageId = generateCompactId('cb-')\n      // no need to await, just send the request and continue\n      sendToRelaceLongContext(normalizedInitialContent, normalizedEditSnippet, {\n        clientSessionId,\n        fingerprintId,\n        userInputId,\n        userId,\n        messageId,\n        userMessage: lastUserPrompt,\n      })\n    }\n    // Delete this later ^^^\n\n    const largeFileContent = await handleLargeFile(\n      normalizedInitialContent,\n      normalizedEditSnippet,\n      clientSessionId,\n      fingerprintId,\n      userInputId,\n      userId,\n      path,\n      costMode\n    )\n\n    if (!largeFileContent) {\n      return null\n    }\n\n    updatedContent = largeFileContent\n  } else {\n    updatedContent = await fastRewrite(\n      normalizedInitialContent,\n      normalizedEditSnippet,\n      path,\n      clientSessionId,\n      fingerprintId,\n      userInputId,\n      userId,\n      lastUserPrompt\n    )\n    const shouldAddPlaceholders = await shouldAddFilePlaceholders(\n      path,\n      normalizedInitialContent,\n      updatedContent,\n      messages,\n      fullResponse,\n      userId,\n      clientSessionId,\n      fingerprintId,\n      userInputId\n    )\n\n    if (shouldAddPlaceholders) {\n      const placeholderComment = `... existing code ...`\n      const updatedEditSnippet = `${placeholderComment}\\n${updatedContent}\\n${placeholderComment}`\n      updatedContent = await fastRewrite(\n        normalizedInitialContent,\n        updatedEditSnippet,\n        path,\n        clientSessionId,\n        fingerprintId,\n        userInputId,\n        userId,\n        lastUserPrompt\n      )\n    }\n  }\n\n  let patch = createPatch(path, normalizedInitialContent, updatedContent)\n  const lines = patch.split('\\n')\n  const hunkStartIndex = lines.findIndex((line) => line.startsWith('@@'))\n  if (hunkStartIndex !== -1) {\n    patch = lines.slice(hunkStartIndex).join('\\n')\n  } else {\n    logger.debug(\n      {\n        path,\n        initialContent,\n        changes: newContent,\n        patch,\n      },\n      `processFileBlock: No change to ${path}`\n    )\n    return null\n  }\n  logger.debug(\n    {\n      path,\n      editSnippet: newContent,\n      updatedContent,\n      patch,\n    },\n    `processFileBlock: Updated file ${path}`\n  )\n\n  const patchOriginalLineEndings = patch.replaceAll('\\n', lineEnding)\n  const updatedContentOriginalLineEndings = updatedContent.replaceAll(\n    '\\n',\n    lineEnding\n  )\n\n  return {\n    tool: 'write_file' as const,\n    path,\n    content: updatedContentOriginalLineEndings,\n    patch: patchOriginalLineEndings,\n  }\n}\n\nconst LARGE_FILE_TOKEN_LIMIT = 16_000\n\nexport async function handleLargeFile(\n  oldContent: string,\n  editSnippet: string,\n  clientSessionId: string,\n  fingerprintId: string,\n  userInputId: string,\n  userId: string | undefined,\n  filePath: string,\n  costMode: CostMode\n): Promise<string | null> {\n  const startTime = Date.now()\n\n  // If the whole file is rewritten, we can just return the new content.\n  if (!hasLazyEdit(editSnippet)) {\n    return editSnippet\n  }\n\n  const prompt =\n    `You are an expert programmer tasked with creating SEARCH/REPLACE blocks to implement a change in a large file. The change should match the intent of the edit snippet while using exact content from the old file.\n\nOld file content:\n\\`\\`\\`\n${oldContent}\n\\`\\`\\`\n\nEdit snippet (the new content to implement):\n\\`\\`\\`\n${editSnippet}\n\\`\\`\\`\n\nPlease analyze the edit snippet and create SEARCH/REPLACE blocks that will transform the old content into the intended new content. The SEARCH content must be an exact substring match from the old file — try to keep the search content as short as possible.\n\nImportant:\n1. The SEARCH content must match exactly to a substring of the old file content - make sure you're using the exact same whitespace, single quotes, double quotes, and backticks.\n2. Keep the changes minimal and focused. Do not include any \"placeholder comments\" (including but not limited to \\`// ... existing code ...\\`) unless you think it should be included in the final output.\n3. Preserve the original formatting, indentation, and comments\n4. Only implement the changes shown in the edit snippet\n\nPlease output just the SEARCH/REPLACE blocks like this:\n\n` +\n    `<<<<<<< SEARCH\n[exact content from old file]\n=======\n[new content that matches edit snippet intent]\n>>>>>>> REPLACE`\n\n  const response = await promptOpenAI([{ role: 'user', content: prompt }], {\n    model: models.o3mini,\n    clientSessionId,\n    fingerprintId,\n    userInputId,\n    userId,\n  })\n\n  const { diffBlocks, diffBlocksThatDidntMatch } =\n    parseAndGetDiffBlocksSingleFile(response, oldContent)\n\n  let updatedContent = oldContent\n  for (const { searchContent, replaceContent } of diffBlocks) {\n    updatedContent = updatedContent.replace(searchContent, replaceContent)\n  }\n\n  if (diffBlocksThatDidntMatch.length > 0) {\n    logger.debug(\n      {\n        duration: Date.now() - startTime,\n        editSnippet,\n        response,\n        diffBlocks,\n        diffBlocksThatDidntMatch,\n        filePath,\n        oldContent,\n      },\n      'Initial diff blocks failed to match, retrying...'\n    )\n\n    const { newDiffBlocks, newDiffBlocksThatDidntMatch } =\n      await retryDiffBlocksPrompt(\n        filePath,\n        updatedContent,\n        costMode,\n        clientSessionId,\n        fingerprintId,\n        userInputId,\n        userId,\n        diffBlocksThatDidntMatch\n      )\n\n    if (newDiffBlocksThatDidntMatch.length > 0) {\n      logger.error(\n        {\n          diffBlocks: newDiffBlocks,\n          diffBlocksThatDidntMatch: newDiffBlocksThatDidntMatch,\n          originalDiffBlocksThatDidntMatch: diffBlocksThatDidntMatch,\n          originalDiffBlocks: diffBlocks,\n          filePath,\n          oldContent,\n          editSnippet,\n          duration: Date.now() - startTime,\n        },\n        'Failed to create matching diff blocks for large file after retry'\n      )\n      return null\n    }\n\n    for (const { searchContent, replaceContent } of newDiffBlocks) {\n      updatedContent = updatedContent.replace(searchContent, replaceContent)\n    }\n  }\n\n  logger.debug(\n    {\n      updatedContent,\n      oldContent,\n      editSnippet,\n      diffBlocks,\n      filePath,\n      duration: Date.now() - startTime,\n    },\n    `handleLargeFile ${filePath}`\n  )\n  return updatedContent\n}\n",
            "tokens": 2743
        },
        "backend/src/process-stream.ts": {
            "content": "export async function* processStreamWithTags<T extends string>(\n  stream: AsyncGenerator<T> | ReadableStream<T>,\n  tags: {\n    [tagName: string]: {\n      attributeNames: string[]\n      onTagStart: (attributes: Record<string, string>) => void\n      onTagEnd: (content: string, attributes: Record<string, string>) => boolean\n    }\n  }\n) {\n  let buffer = ''\n  let insideTag: string | null = null\n  let currentAttributes: Record<string, string> = {}\n  let streamCompleted = false\n\n  const escapeRegExp = (string: string) =>\n    string.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&')\n  const tagNames = Object.keys(tags)\n  const openTagRegex = new RegExp(\n    `<(${tagNames.map(escapeRegExp).join('|')})\\\\s*([^>]*)>`\n  )\n  const closeTagRegex = new RegExp(\n    `</(${tagNames.map(escapeRegExp).join('|')})>`\n  )\n\n  function* parseBuffer(\n    chunk: string | undefined\n  ): Generator<string, void, unknown> {\n    const isEOF = chunk === undefined\n    if (chunk) {\n      yield chunk\n    }\n    let didParse = true\n\n    while (!streamCompleted && didParse) {\n      didParse = false\n\n      if (insideTag === null) {\n        // Outside a tag: try to find the next opening tag\n        const openMatch = buffer.match(openTagRegex)\n        if (openMatch && openMatch.index !== undefined) {\n          const [fullMatch, openTag, attributesString] = openMatch\n          const beforeTag = buffer.slice(0, openMatch.index)\n          const afterMatchIndex = openMatch.index + fullMatch.length\n\n          // Move buffer forward\n          buffer = buffer.slice(afterMatchIndex)\n\n          // We are now inside this tag\n          insideTag = openTag\n          currentAttributes = parseAttributes(\n            attributesString,\n            tags[openTag].attributeNames\n          )\n\n          // Call onTagStart\n          tags[openTag].onTagStart(currentAttributes)\n\n          didParse = true\n        } else {\n          // No opening tag found. If it's EOF, yield remaining text.\n          if (isEOF && buffer.length > 0) {\n            buffer = ''\n          }\n        }\n      } else {\n        // Inside a tag: try to find the closing tag\n        const closeMatch = buffer.match(closeTagRegex)\n        if (closeMatch && closeMatch.index !== undefined) {\n          const [fullMatch, closeTag] = closeMatch\n          const content = buffer.slice(0, closeMatch.index)\n\n          // Move buffer forward\n          buffer = buffer.slice(closeMatch.index + fullMatch.length)\n\n          // Close the tag\n          const complete = tags[insideTag].onTagEnd(content, currentAttributes)\n          insideTag = null\n          currentAttributes = {}\n\n          if (complete) {\n            // If onTagEnd signals completion, set streamCompleted and return\n            streamCompleted = true\n            return\n          }\n\n          didParse = true\n        } else if (isEOF) {\n          // We reached EOF without finding a closing tag\n          // Treat remaining buffer as content and close the tag\n          const complete = tags[insideTag].onTagEnd(buffer, currentAttributes)\n          yield '</' + insideTag + '>'\n          buffer = ''\n          insideTag = null\n          currentAttributes = {}\n          if (complete) {\n            streamCompleted = true\n            return\n          }\n        }\n      }\n    }\n  }\n\n  for await (const chunk of stream) {\n    if (streamCompleted) break\n    buffer += chunk\n    yield* parseBuffer(chunk)\n  }\n\n  if (!streamCompleted) {\n    // After the stream ends, try parsing one last time in case there's leftover text\n    yield* parseBuffer(undefined)\n  }\n}\n\n// exported for tests\nexport function parseAttributes(\n  attributesString: string,\n  attributeNames: string[]\n): Record<string, string> {\n  const attributes: Record<string, string> = {}\n  const regex = new RegExp(\n    `(${attributeNames.join('|')})\\\\s*=\\\\s*\"([^\"]*)\"`,\n    'g'\n  )\n  let match\n  while ((match = regex.exec(attributesString)) !== null) {\n    attributes[match[1]] = match[2]\n  }\n  return attributes\n}\n",
            "tokens": 1264
        },
        "backend/src/system-prompt/agent-system-prompt.ts": {
            "content": "import fs from 'fs'\nimport path from 'path'\n\nimport { buildArray } from 'common/util/array'\nimport { ProjectFileContext } from 'common/util/file'\nimport { countTokens } from 'gpt-tokenizer'\n\nimport { toolsInstructions } from '../tools'\nimport { logger } from '../util/logger'\nimport { countTokensJson } from '../util/token-counter'\nimport {\n  configSchemaPrompt,\n  getGitChangesPrompt,\n  getProjectFileTreePrompt,\n  getSystemInfoPrompt,\n  knowledgeFilesPrompt,\n} from './prompts'\n\nexport const getAgentSystemPrompt = (fileContext: ProjectFileContext) => {\n  const agentInstructions = fs.readFileSync(\n    path.join(__dirname, 'agent-instructions.md'),\n    'utf8'\n  )\n\n  const startTime = Date.now()\n  // Agent token budget:\n  // System prompt stuff, git changes: 25k\n  // Files: 100k (25k for lite)\n  // File tree: 20k (5k for lite)\n  // Messages: Remaining\n  // Total: 200k (64k for lite)\n\n  const gitChangesPrompt = getGitChangesPrompt(fileContext)\n  const gitChangesTokens = countTokensJson(gitChangesPrompt)\n  const fileTreeTokenBudget = 20_000 //costMode === 'lite' ? 5_000 :\n\n  const projectFileTreePrompt = getProjectFileTreePrompt(\n    fileContext,\n    fileTreeTokenBudget,\n    'agent'\n  )\n  const fileTreeTokens = countTokensJson(projectFileTreePrompt)\n\n  const systemInfoPrompt = getSystemInfoPrompt(fileContext)\n  const systemInfoTokens = countTokens(systemInfoPrompt)\n\n  const configSchemaTokens = countTokens(configSchemaPrompt)\n\n  const systemPrompt = buildArray(\n    agentInstructions,\n    toolsInstructions,\n    knowledgeFilesPrompt,\n    configSchemaPrompt,\n    projectFileTreePrompt,\n    systemInfoPrompt,\n    gitChangesPrompt\n  ).map((prompt) => ({\n    type: 'text' as const,\n    text: prompt,\n  }))\n\n  logger.debug(\n    {\n      fileTreeTokens,\n      fileTreeTokenBudget,\n      systemInfoTokens,\n      configSchemaTokens,\n      systemPromptTokens: countTokensJson(systemPrompt),\n      gitChangesTokens,\n      duration: Date.now() - startTime,\n    },\n    'agent system prompt tokens'\n  )\n\n  return systemPrompt\n}\n",
            "tokens": 677
        },
        "backend/src/util/messages.ts": {
            "content": "import { Message } from 'common/types/message'\nimport { withCacheControl } from 'common/util/messages'\n\nimport { System } from '../llm-apis/claude'\nimport { OpenAIMessage } from '../llm-apis/openai-api'\nimport { logger } from './logger'\nimport { simplifyTerminalCommandResults } from './simplify-tool-results'\nimport { countTokensJson } from './token-counter'\n\n/**\n * Wraps an array of messages with a system prompt for LLM API calls\n * @param messages - Array of messages to wrap\n * @param system - System prompt to prepend\n * @returns Array with system message followed by provided messages\n */\nexport const messagesWithSystem = (messages: Message[], system: System) =>\n  [{ role: 'system', content: system }, ...messages] as OpenAIMessage[]\n\nexport function asSystemInstruction(str: string): string {\n  return `<system_instructions>${str}</system_instructions>`\n}\n\nexport function asSystemMessage(str: string): string {\n  return `<system>${str}</system>`\n}\n\nexport function isSystemInstruction(str: string): boolean {\n  return (\n    str.startsWith('<system_instructions>') &&\n    str.endsWith('</system_instructions>')\n  )\n}\n\nexport function isSystemMessage(str: string): boolean {\n  return str.startsWith('<system>') && str.endsWith('</system>')\n}\n\n/**\n * Extracts the text content from a message, handling both string and array content types\n * @param message - Message to extract text from\n * @returns Combined text content of the message, or undefined if no text content\n */\nexport function getMessageText(message: Message): string | undefined {\n  if (typeof message.content === 'string') {\n    return message.content\n  }\n  return message.content.map((c) => ('text' in c ? c.text : '')).join('\\n')\n}\n\n// Number of terminal command outputs to keep in full form before simplifying\nconst numTerminalCommandsToKeep = 5\n\n/**\n * Helper function to simplify terminal command output while preserving some recent ones\n * @param text - Terminal output text to potentially simplify\n * @param numKept - Number of terminal outputs already kept in full form\n * @returns Object containing simplified result and updated count of kept outputs\n */\nfunction simplifyTerminalHelper(\n  text: string,\n  numKept: number\n): { result: string; numKept: number } {\n  const simplifiedText = simplifyTerminalCommandResults(text)\n\n  // Keep the full output for the N most recent commands\n  if (numKept < numTerminalCommandsToKeep && simplifiedText !== text) {\n    return { result: text, numKept: numKept + 1 }\n  }\n\n  return {\n    result: simplifiedText,\n    numKept,\n  }\n}\n\n// Factor to reduce token count target by, to leave room for new messages\nconst shortenedMessageTokenFactor = 0.5\n\n/**\n * Trims messages from the beginning to fit within token limits while preserving\n * important content. Also simplifies terminal command outputs to save tokens.\n *\n * The function:\n * 1. Processes messages from newest to oldest\n * 2. Simplifies terminal command outputs after keeping N most recent ones\n * 3. Stops adding messages when approaching token limit\n *\n * @param messages - Array of messages to trim\n * @param systemTokens - Number of tokens used by system prompt\n * @param maxTotalTokens - Maximum total tokens allowed, defaults to 200k\n * @returns Trimmed array of messages that fits within token limit\n */\nexport function trimMessagesToFitTokenLimit(\n  messages: Message[],\n  systemTokens: number,\n  maxTotalTokens: number = 200_000\n): Message[] {\n  const MAX_MESSAGE_TOKENS = maxTotalTokens - systemTokens\n\n  // Check if we're already under the limit\n  const initialTokens = countTokensJson(messages)\n\n  if (initialTokens < MAX_MESSAGE_TOKENS) {\n    return messages\n  }\n\n  let totalTokens = 0\n  const targetTokens = MAX_MESSAGE_TOKENS * shortenedMessageTokenFactor\n  const results: Message[] = []\n  let numKept = 0\n\n  // Process messages from newest to oldest\n  for (let i = messages.length - 1; i >= 0; i--) {\n    const { role, content } = messages[i]\n    let newContent: typeof content\n\n    // Handle string content (usually terminal output)\n    if (typeof content === 'string') {\n      if (isSystemInstruction(content)) {\n        continue\n      }\n      const result = simplifyTerminalHelper(content, numKept)\n      newContent = result.result\n      numKept = result.numKept\n    } else {\n      // Handle array content (mixed content types)\n      newContent = []\n      // Process content parts from newest to oldest\n      for (let j = content.length - 1; j >= 0; j--) {\n        const messagePart = content[j]\n        // Preserve non-text content (i.e. images)\n        if (messagePart.type !== 'text') {\n          newContent.push(messagePart)\n          continue\n        }\n\n        const result = simplifyTerminalHelper(messagePart.text, numKept)\n        newContent.push({ ...messagePart, text: result.result })\n        numKept = result.numKept\n      }\n      newContent.reverse()\n    }\n\n    // Check if adding this message would exceed our token target\n    const message = { role, content: newContent }\n    const messageTokens = countTokensJson(message)\n\n    if (totalTokens + messageTokens <= targetTokens) {\n      results.push({ role, content: newContent })\n      totalTokens += messageTokens\n    } else {\n      break\n    }\n  }\n\n  results.reverse()\n  return results\n}\n\nexport function getMessagesSubset(messages: Message[], otherTokens: number) {\n  const indexLastSubgoalComplete = messages.findLastIndex(({ content }) => {\n    JSON.stringify(content).includes('COMPLETE')\n  })\n\n  const messagesSubset = trimMessagesToFitTokenLimit(\n    indexLastSubgoalComplete === -1\n      ? messages\n      : messages.slice(indexLastSubgoalComplete),\n    otherTokens\n  )\n\n  // Remove cache_control from all messages\n  for (const message of messagesSubset) {\n    if (typeof message.content === 'object' && message.content.length > 0) {\n      delete message.content[message.content.length - 1].cache_control\n    }\n  }\n\n  // Cache up to the last message!\n  const lastMessage = messagesSubset[messagesSubset.length - 1]\n  if (lastMessage) {\n    messagesSubset[messagesSubset.length - 1] = withCacheControl(lastMessage)\n  } else {\n    logger.debug(\n      {\n        messages,\n        messagesSubset,\n        otherTokens,\n      },\n      'No last message found in messagesSubset!'\n    )\n  }\n\n  return messagesSubset\n}\n",
            "tokens": 1949
        },
        "backend/src/util/parse-tool-call-xml.ts": {
            "content": "import { ToolResult } from 'common/types/agent-state'\nimport { Message } from 'common/types/message'\nimport { toContentString } from 'common/util/messages'\nimport { generateCompactId } from 'common/util/string'\n\n/**\n * Parses XML content for a tool call into a structured object with only string values.\n * Example input:\n * <type>click</type>\n * <selector>#button</selector>\n * <timeout>5000</timeout>\n */\nexport function parseToolCallXml(xmlString: string): Record<string, string> {\n  if (!xmlString.trim()) return {}\n\n  const result: Record<string, string> = {}\n  const tagPattern = /<(\\w+)>([\\s\\S]*?)<\\/\\1>/g\n  let match\n\n  while ((match = tagPattern.exec(xmlString)) !== null) {\n    const [_, key, rawValue] = match\n\n    // Remove leading/trailing whitespace but preserve internal whitespace\n    const value = rawValue.replace(/^\\s+|\\s+$/g, '')\n\n    // Assign all values as strings\n    result[key] = value\n  }\n\n  return result\n}\n\nexport function renderToolResults(toolResults: ToolResult[]): string {\n  return `\n${toolResults\n  .map(\n    (result) => `<tool_result>\n<tool>${result.name}</tool>\n<result>${result.result}</result>\n</tool_result>`\n  )\n  .join('\\n\\n')}\n`.trim()\n}\n\nexport const parseToolResults = (xmlString: string): ToolResult[] => {\n  if (!xmlString.trim()) return []\n\n  const results: ToolResult[] = []\n  const toolResultPattern = /<tool_result>([\\s\\S]*?)<\\/tool_result>/g\n  let match\n\n  while ((match = toolResultPattern.exec(xmlString)) !== null) {\n    const [_, toolResultContent] = match\n    const toolMatch = /<tool>(.*?)<\\/tool>/g.exec(toolResultContent)\n    const resultMatch = /<result>([\\s\\S]*?)<\\/result>/g.exec(toolResultContent)\n\n    if (toolMatch && resultMatch) {\n      results.push({\n        id: generateCompactId(),\n        name: toolMatch[1],\n        result: resultMatch[1].trim(),\n      })\n    }\n  }\n\n  return results\n}\n\nexport function renderReadFilesResult(\n  files: { path: string; content: string }[]\n) {\n  return files\n    .map(\n      (file) =>\n        `<read_file>\\n<path>${file.path}</path>\\n<content>${file.content}</content>\\n</read_file>`\n    )\n    .join('\\n\\n')\n}\n\nexport function parseReadFilesResult(\n  xmlString: string\n): { path: string; content: string }[] {\n  const files: { path: string; content: string }[] = []\n  const filePattern =\n    /<read_file>\\n<path>([^<>]+)<\\/path>\\n<content>([\\s\\S]*?)<\\/content>\\n<\\/read_file>/g\n  let match\n\n  while ((match = filePattern.exec(xmlString)) !== null) {\n    const [, filePath, content] = match\n    if (filePath.trim()) {\n      files.push({ path: filePath.trim(), content })\n    }\n  }\n\n  return files\n}\n\nexport function isToolResult(message: Message): boolean {\n  return toContentString(message).includes('<tool_result')\n}\n",
            "tokens": 974
        },
        "backend/src/util/simplify-tool-results.ts": {
            "content": "import { ToolResult } from 'common/types/agent-state'\n\nimport {\n  parseToolResults,\n  renderToolResults,\n  parseReadFilesResult,\n} from './parse-tool-call-xml'\n\n/**\n * Helper function to simplify tool results of a specific type while preserving others.\n * Extracts results of the specified tool type, applies a simplification function to them,\n * and combines them back with other unchanged tool results.\n * @param messageContent - The message content containing tool results, either as a string or array\n * @param toolName - The name of the tool whose results should be simplified\n * @param simplifyFn - Function to apply to each matching tool result\n * @returns The message content with simplified results for the specified tool type\n */\nfunction simplifyToolResults(\n  messageContent: string | object[],\n  toolName: string,\n  simplifyFn: (result: ToolResult) => ToolResult\n): string {\n  const resultsStr =\n    typeof messageContent === 'string'\n      ? messageContent\n      : ((messageContent[messageContent.length - 1] as any)?.text as string) ??\n        ''\n  if (!resultsStr.includes('<tool_result')) {\n    return resultsStr\n  }\n\n  const toolResults = parseToolResults(resultsStr)\n  const targetResults = toolResults.filter((result) => result.name === toolName)\n\n  if (targetResults.length === 0) {\n    return resultsStr\n  }\n\n  // Keep non-target results unchanged\n  const otherResults = toolResults.filter((result) => result.name !== toolName)\n\n  // Create simplified results\n  const simplifiedResults = targetResults.map(simplifyFn)\n\n  // Combine both types of results\n  return renderToolResults([...simplifiedResults, ...otherResults])\n}\n\n/**\n * Simplifies read_files tool results to show only file paths while preserving other tool results.\n * Useful for making tool result output more concise in message history.\n * @param messageContent - The message content containing tool results\n * @returns The message content with simplified read_files results showing only paths\n */\nexport function simplifyReadFileResults(\n  messageContent: string | object[]\n): string {\n  return simplifyToolResults(\n    messageContent,\n    'read_files',\n    simplifyReadFileToolResult\n  )\n}\n\n/**\n * Simplifies terminal command tool results to show a brief summary while preserving other tool results.\n * Useful for making tool result output more concise in message history.\n * @param messageContent - The message content containing tool results\n * @returns The message content with simplified terminal command results\n */\nexport function simplifyTerminalCommandResults(\n  messageContent: string | object[]\n): string {\n  return simplifyToolResults(\n    messageContent,\n    'run_terminal_command',\n    simplifyTerminalCommandToolResult\n  )\n}\n\n/**\n * Simplifies a single read_files tool result by extracting just the file paths.\n * @param toolResult - The read_files tool result to simplify\n * @returns A new tool result with just the list of file paths that were read\n */\nexport function simplifyReadFileToolResult(toolResult: ToolResult): ToolResult {\n  const fileBlocks = parseReadFilesResult(toolResult.result)\n  const filePaths = fileBlocks.map((block) => block.path)\n  return {\n    id: toolResult.id,\n    name: 'read_files',\n    result: `Read the following files: ${filePaths.join('\\n')}`,\n  }\n}\n\n/**\n * Simplifies a single terminal command tool result by replacing output with a brief message.\n * @param toolResult - The terminal command tool result to simplify\n * @returns A new tool result with shortened output if the original was long\n */\nexport function simplifyTerminalCommandToolResult(\n  toolResult: ToolResult\n): ToolResult {\n  const shortenedResultCandidate = '[Output omitted]'\n  return shortenedResultCandidate.length < toolResult.result.length\n    ? {\n        id: toolResult.id,\n        name: 'run_terminal_command',\n        result: shortenedResultCandidate,\n      }\n    : toolResult\n}\n",
            "tokens": 1111
        },
        "backend/src/util/split-data.ts": {
            "content": "type PlainObject = Record<string, any>\n\ninterface Chunk<T> {\n  data: T\n  length: number\n}\n\nfunction isPlainObject(val: any): val is PlainObject {\n  return (\n    typeof val === 'object' &&\n    val !== null &&\n    Object.getPrototypeOf(val) === Object.prototype\n  )\n}\n\nfunction getJsonSize(data: any): number {\n  if (data === undefined) {\n    return 'undefined'.length\n  }\n  const size = JSON.stringify(data).length\n  return size\n}\n\nfunction splitString(data: string, maxSize: number): Chunk<string>[] {\n  if (data === '') {\n    return [{ data: '', length: 2 }]\n  }\n\n  const chunks: Chunk<string>[] = []\n  let currentChunk: Chunk<string> = { data: '', length: 2 }\n\n  if (maxSize < 2) {\n    for (let i = 0; i < data.length; i++) {\n      chunks.push({ data: data[i], length: getJsonSize(data[i]) })\n    }\n    return chunks\n  }\n\n  for (let i = 0; i < data.length; i++) {\n    const char = data[i]\n    const charSizeContribution = JSON.stringify(char).length - 2\n    let potentialNextSize: number\n\n    potentialNextSize = currentChunk.length + charSizeContribution\n\n    if (potentialNextSize <= maxSize) {\n      currentChunk.data += char\n      currentChunk.length = potentialNextSize\n    } else {\n      if (currentChunk.data !== '') {\n        chunks.push(currentChunk)\n      }\n\n      currentChunk = { data: char, length: 2 + charSizeContribution }\n    }\n  }\n\n  if (currentChunk.data !== '') {\n    chunks.push(currentChunk)\n  }\n\n  return chunks\n}\n\nfunction splitObject(obj: PlainObject, maxSize: number): Chunk<PlainObject>[] {\n  const chunks: Chunk<PlainObject>[] = []\n\n  let currentChunk: Chunk<PlainObject> = {\n    data: {},\n    length: 2,\n  }\n  for (const [key, value] of Object.entries(obj)) {\n    const entryObject = { [key]: value }\n    const standaloneEntry: Chunk<PlainObject> = {\n      data: entryObject,\n      length: getJsonSize(entryObject),\n    }\n\n    if (standaloneEntry.length > maxSize) {\n      const overhead = getJsonSize({ [key]: '' }) - 2\n\n      const items = splitDataWithLengths(\n        value,\n        maxSize - (getJsonSize({ [key]: '' }) - 2)\n      )\n\n      for (const [index, item] of items.entries()) {\n        const itemWithKey: Chunk<any> = {\n          data: { [key]: item.data },\n          length: item.length + overhead,\n        }\n\n        if (index < items.length - 1) {\n          if (key in currentChunk.data) {\n            chunks.push(currentChunk)\n            currentChunk = itemWithKey\n            continue\n          }\n\n          const candidateChunkLength =\n            currentChunk.length +\n            itemWithKey.length -\n            (currentChunk.length === 2 ? 2 : 3)\n          if (candidateChunkLength <= maxSize) {\n            currentChunk.data[key] = item.data\n            currentChunk.length = candidateChunkLength\n            continue\n          }\n\n          if (currentChunk.length > 2) {\n            chunks.push(currentChunk)\n          }\n          currentChunk = itemWithKey\n          continue\n        }\n\n        if (currentChunk.length > 2) {\n          chunks.push(currentChunk)\n        }\n        currentChunk = itemWithKey\n      }\n\n      continue\n    }\n\n    const candidateChunkLength =\n      currentChunk.length +\n      standaloneEntry.length -\n      (currentChunk.length === 2 ? 2 : 3)\n\n    if (candidateChunkLength <= maxSize) {\n      currentChunk.data[key] = value\n      currentChunk.length = candidateChunkLength\n      continue\n    }\n\n    if (currentChunk.length > 2) {\n      chunks.push(currentChunk)\n      currentChunk = standaloneEntry\n    }\n  }\n\n  if (currentChunk.length > 2) {\n    chunks.push(currentChunk)\n  }\n\n  return chunks\n}\n\nfunction splitArray(arr: any[], maxSize: number): Chunk<any[]>[] {\n  const chunks: Chunk<any[]>[] = []\n  let currentChunk: Chunk<any[]> = { data: [], length: 2 }\n\n  for (const element of arr) {\n    const entryArr = [element]\n    const standaloneEntry: Chunk<any[]> = {\n      data: entryArr,\n      length: getJsonSize(entryArr),\n    }\n\n    if (standaloneEntry.length > maxSize) {\n      if (currentChunk.length > 2) {\n        chunks.push(currentChunk)\n      }\n\n      const items = splitDataWithLengths(element, maxSize - 2)\n\n      for (const [index, item] of items.entries()) {\n        if (index < items.length - 1) {\n          // Try to add to current chunk\n          const candidateChunkLength =\n            currentChunk.length +\n            item.length +\n            (currentChunk.length === 2 ? 1 : 0)\n          if (candidateChunkLength <= maxSize) {\n            currentChunk.data.push(item.data)\n            currentChunk.length = candidateChunkLength\n            continue\n          }\n\n          chunks.push({ data: [item.data], length: item.length + 2 })\n          continue\n        }\n\n        currentChunk = { data: [item.data], length: item.length + 2 }\n      }\n      continue\n    }\n\n    const candidateChunkLength =\n      currentChunk.length +\n      standaloneEntry.length -\n      (currentChunk.length === 2 ? 1 : 2)\n\n    if (candidateChunkLength <= maxSize) {\n      currentChunk.data.push(element)\n      currentChunk.length = candidateChunkLength\n      continue\n    }\n\n    if (currentChunk.length > 2) {\n      chunks.push(currentChunk)\n      currentChunk = standaloneEntry\n    }\n  }\n\n  if (currentChunk.length > 2) {\n    chunks.push(currentChunk)\n  }\n\n  return chunks\n}\n\nfunction splitDataWithLengths(data: any, maxChunkSize: number): Chunk<any>[] {\n  // Handle primitives\n  if (typeof data !== 'object' || data === null) {\n    if (typeof data === 'string') {\n      const result = splitString(data, maxChunkSize)\n      return result\n    }\n    return [{ data, length: getJsonSize(data) }]\n  }\n\n  // Non-plain objects (Date, RegExp, etc.)\n  if (!Array.isArray(data) && !isPlainObject(data)) {\n    return [{ data, length: getJsonSize(data) }]\n  }\n\n  // Arrays\n  if (Array.isArray(data)) {\n    const result = splitArray(data, maxChunkSize)\n    return result\n  }\n\n  // Plain objects\n  const result = splitObject(data, maxChunkSize)\n  return result\n}\n\nexport function splitData(data: any, maxChunkSize: number = 99_000): any[] {\n  return splitDataWithLengths(data, maxChunkSize).map((cwjl) => cwjl.data)\n}\n",
            "tokens": 2034
        },
        "backend/src/util/token-counter.ts": {
            "content": "import { encode } from 'gpt-tokenizer/esm/model/gpt-4o'\nimport { LRUCache } from 'common/util/lru-cache'\n\nconst ANTHROPIC_TOKEN_FUDGE_FACTOR = 1.35\n\nconst TOKEN_COUNT_CACHE = new LRUCache<string, number>(1000)\n\nexport function countTokens(text: string): number {\n  try {\n    const cached = TOKEN_COUNT_CACHE.get(text)\n    if (cached !== undefined) {\n      return cached\n    }\n    const count = Math.floor(encode(text).length * ANTHROPIC_TOKEN_FUDGE_FACTOR)\n\n    if (text.length > 100) {\n      // Cache only if the text is long enough to be worth it.\n      TOKEN_COUNT_CACHE.set(text, count)\n    }\n    return count\n  } catch (e) {\n    console.error('Error counting tokens', e)\n    return Math.ceil(text.length / 3)\n  }\n}\n\nexport function countTokensJson(text: string | object): number {\n  return countTokens(JSON.stringify(text))\n}\n\nexport function countTokensForFiles(\n  files: Record<string, string | null>\n): Record<string, number> {\n  const tokenCounts: Record<string, number> = {}\n  for (const [filePath, content] of Object.entries(files)) {\n    tokenCounts[filePath] = content ? countTokens(content) : 0\n  }\n  return tokenCounts\n}\n",
            "tokens": 392
        },
        "backend/src/websockets/middleware.ts": {
            "content": "import { ClientAction, ServerAction } from 'common/actions'\nimport { WebSocket } from 'ws'\n\nimport { checkAuth } from '../util/check-auth'\nimport { logger, withLoggerContext } from '@/util/logger'\nimport { getUserInfoFromAuthToken, UserInfo } from './auth'\nimport { sendAction } from './websocket-action'\nimport {\n  calculateUsageAndBalance,\n  triggerMonthlyResetAndGrant,\n  checkAndTriggerAutoTopup,\n} from '@codebuff/billing'\nimport db from 'common/db'\nimport * as schema from 'common/db/schema'\nimport { eq } from 'drizzle-orm'\nimport { pluralize } from 'common/util/string'\nimport { env } from '@/env.mjs'\n\ntype MiddlewareCallback = (\n  action: ClientAction,\n  clientSessionId: string,\n  ws: WebSocket,\n  userInfo: UserInfo | undefined\n) => Promise<void | ServerAction>\n\nexport class WebSocketMiddleware {\n  private middlewares: Array<MiddlewareCallback> = []\n\n  use<T extends ClientAction['type']>(\n    callback: (\n      action: Extract<ClientAction, { type: T }>,\n      clientSessionId: string,\n      ws: WebSocket,\n      userInfo: UserInfo | undefined\n    ) => Promise<void | ServerAction>\n  ) {\n    this.middlewares.push(callback as MiddlewareCallback)\n  }\n\n  async execute(\n    action: ClientAction,\n    clientSessionId: string,\n    ws: WebSocket,\n    options: { silent?: boolean } = {}\n  ): Promise<boolean> {\n    const userInfo =\n      'authToken' in action && action.authToken\n        ? await getUserInfoFromAuthToken(action.authToken)\n        : undefined\n\n    return await withLoggerContext(\n      {\n        clientSessionId,\n        userId: userInfo?.id,\n        userEmail: userInfo?.email,\n        discordId: userInfo?.discord_id ?? undefined,\n      },\n      async () => {\n        for (const middleware of this.middlewares) {\n          const actionOrContinue = await middleware(\n            action,\n            clientSessionId,\n            ws,\n            userInfo\n          )\n          if (actionOrContinue) {\n            logger.warn(\n              {\n                actionType: action.type,\n                middlewareResp: actionOrContinue.type,\n                clientSessionId,\n              },\n              'Middleware execution halted.'\n            )\n            if (!options.silent) {\n              sendAction(ws, actionOrContinue)\n            }\n            return false\n          }\n        }\n        return true\n      }\n    )\n  }\n\n  run<T extends ClientAction['type']>(\n    baseAction: (\n      action: Extract<ClientAction, { type: T }>,\n      clientSessionId: string,\n      ws: WebSocket\n    ) => void,\n    options: { silent?: boolean } = {}\n  ) {\n    return async (\n      action: Extract<ClientAction, { type: T }>,\n      clientSessionId: string,\n      ws: WebSocket\n    ) => {\n      const userInfo =\n        'authToken' in action\n          ? await getUserInfoFromAuthToken(action.authToken!)\n          : undefined\n\n      return withLoggerContext(\n        {\n          clientSessionId,\n          userId: userInfo?.id,\n          userEmail: userInfo?.email,\n          discordId: userInfo?.discord_id ?? undefined,\n        },\n        async () => {\n          const shouldContinue = await this.execute(\n            action,\n            clientSessionId,\n            ws,\n            options\n          )\n          if (shouldContinue) {\n            baseAction(action, clientSessionId, ws)\n          }\n        }\n      )\n    }\n  }\n}\n\nexport const protec = new WebSocketMiddleware()\n\nprotec.use(async (action, clientSessionId, ws, userInfo) =>\n  checkAuth({\n    fingerprintId: 'fingerprintId' in action ? action.fingerprintId : undefined,\n    authToken: 'authToken' in action ? action.authToken : undefined,\n    clientSessionId,\n  })\n)\n\nprotec.use(async (action, clientSessionId, ws, userInfo) => {\n  const userId = userInfo?.id\n  const fingerprintId =\n    'fingerprintId' in action ? action.fingerprintId : 'unknown-fingerprint'\n\n  if (!userId || !fingerprintId) {\n    logger.warn(\n      {\n        userId,\n        fingerprintId,\n        actionType: action.type,\n      },\n      'Missing user or fingerprint ID'\n    )\n    return {\n      type: 'action-error',\n      error: 'Missing user or fingerprint ID',\n      message: 'Please log in to continue.',\n    }\n  }\n\n  // Get user info for balance calculation\n  const user = await db.query.user.findFirst({\n    where: eq(schema.user.id, userId),\n    columns: {\n      next_quota_reset: true,\n      stripe_customer_id: true,\n    },\n  })\n\n  // Check and trigger monthly reset if needed\n  await triggerMonthlyResetAndGrant(userId)\n\n  // Check if we need to trigger auto top-up and get the amount added (if any)\n  let autoTopupAdded: number | undefined = undefined\n  try {\n    autoTopupAdded = await checkAndTriggerAutoTopup(userId)\n  } catch (error) {\n    logger.error(\n      { error, userId, clientSessionId },\n      'Error during auto top-up check in middleware'\n    )\n    // Continue execution to check remaining balance\n  }\n\n  const { usageThisCycle, balance } = await calculateUsageAndBalance(\n    userId,\n    user?.next_quota_reset ?? new Date(0)\n  )\n\n  // Check if we have enough remaining credits\n  if (balance.totalRemaining <= 0) {\n    // If they have debt, show that in the message\n    const message =\n      balance.totalDebt > 0\n        ? `You have a balance of negative ${pluralize(Math.abs(balance.totalDebt), 'credit')}. Please add credits to continue using Codebuff.`\n        : `You do not have enough credits for this action. Please add credits or wait for your next cycle to begin.`\n\n    return {\n      type: 'action-error',\n      error: 'Insufficient credits',\n      message,\n      remainingBalance: balance.netBalance,\n    }\n  }\n\n  // Send initial usage info if we have sufficient credits\n  sendAction(ws, {\n    type: 'usage-response',\n    usage: usageThisCycle,\n    remainingBalance: balance.totalRemaining,\n    balanceBreakdown: balance.breakdown,\n    next_quota_reset: user?.next_quota_reset ?? null,\n    autoTopupAdded, // Include the amount added by auto top-up (if any)\n  })\n\n  return undefined\n})\n",
            "tokens": 1883
        },
        "backend/src/websockets/server.ts": {
            "content": "import { Server as HttpServer } from 'node:http'\n\nimport {\n  CLIENT_MESSAGE_SCHEMA,\n  ServerMessage,\n} from 'common/websockets/websocket-schema'\nimport { isError } from 'lodash'\nimport { RawData, WebSocket, Server as WebSocketServer } from 'ws'\n\nimport { logger } from '../util/logger'\nimport { Switchboard } from './switchboard'\nimport { onWebsocketAction } from './websocket-action'\n\nexport const SWITCHBOARD = new Switchboard()\n\n// if a connection doesn't ping for this long, we assume the other side is toast\nconst CONNECTION_TIMEOUT_MS = 60 * 1000\n\nexport class MessageParseError extends Error {\n  details?: unknown\n  constructor(message: string, details?: unknown) {\n    super(message)\n    this.name = 'MessageParseError'\n    this.details = details\n  }\n}\n\nfunction serializeError(err: unknown) {\n  return isError(err) ? err.message : 'Unexpected error.'\n}\n\nasync function processMessage(\n  ws: WebSocket,\n  clientSessionId: string,\n  data: RawData\n): Promise<ServerMessage<'ack'>> {\n  let messageObj: any\n  try {\n    messageObj = JSON.parse(data.toString())\n  } catch (err) {\n    logger.error(\n      { err, data },\n      'Error parsing message: not valid UTF-8 encoded JSON.'\n    )\n    return { type: 'ack', success: false, error: serializeError(err) }\n  }\n\n  try {\n    const msg = CLIENT_MESSAGE_SCHEMA.parse(messageObj)\n    const { type, txid } = msg\n    switch (type) {\n      case 'subscribe': {\n        SWITCHBOARD.subscribe(ws, ...msg.topics)\n        break\n      }\n      case 'unsubscribe': {\n        SWITCHBOARD.unsubscribe(ws, ...msg.topics)\n        break\n      }\n      case 'ping': {\n        SWITCHBOARD.markSeen(ws)\n        break\n      }\n      case 'action': {\n        onWebsocketAction(ws, clientSessionId, msg)\n        break\n      }\n      default:\n        throw new Error(\"Unknown message type; shouldn't be possible here.\")\n    }\n    return { type: 'ack', txid, success: true }\n  } catch (err) {\n    logger.error({ err }, 'Error processing message')\n    return {\n      type: 'ack',\n      txid: messageObj.txid,\n      success: false,\n      error: serializeError(err),\n    }\n  }\n}\n\nexport function listen(server: HttpServer, path: string) {\n  logger.info(`Listening on websocket path: ${path}`)\n  const wss = new WebSocketServer({ server, path })\n  let deadConnectionCleaner: any | undefined\n  wss.on('listening', () => {\n    logger.info(`Web socket server listening on ${path}.`)\n    deadConnectionCleaner = setInterval(function ping() {\n      const now = Date.now()\n      try {\n        for (const ws of wss.clients) {\n          try {\n            const client = SWITCHBOARD.getClient(ws)\n            if (!client) {\n              logger.warn(\n                'Client not found in switchboard, terminating connection'\n              )\n              ws.terminate()\n              continue\n            }\n\n            const lastSeen = client.lastSeen\n            if (lastSeen < now - CONNECTION_TIMEOUT_MS) {\n              ws.terminate()\n            }\n          } catch (err) {\n            // logger.error(\n            //   { error: err },\n            //   'Error checking individual connection in deadConnectionCleaner'\n            // )\n          }\n        }\n      } catch (error) {\n        logger.error({ error }, 'Error in deadConnectionCleaner outer loop')\n      }\n    }, CONNECTION_TIMEOUT_MS)\n  })\n  wss.on('error', (err) => {\n    logger.error({ error: err }, 'Error on websocket server.')\n  })\n  wss.on('connection', (ws) => {\n    // todo: should likely kill connections that haven't sent any ping for a long time\n    // logger.info('WS client connected.')\n    SWITCHBOARD.connect(ws)\n    const clientSessionId =\n      SWITCHBOARD.clients.get(ws)?.sessionId ?? 'mc-client-unknown'\n    ws.on('message', async (data) => {\n      const result = await processMessage(ws, clientSessionId, data)\n      // mqp: check ws.readyState before sending?\n      ws.send(JSON.stringify(result))\n    })\n    ws.on('close', (code, reason) => {\n      // logger.debug(\n      //   { code, reason: reason.toString() },\n      //   'WS client disconnected.'\n      // )\n      SWITCHBOARD.disconnect(ws)\n    })\n    ws.on('error', (err) => {\n      logger.error({ error: err }, 'Error on websocket connection.')\n    })\n  })\n  wss.on('close', function close() {\n    clearInterval(deadConnectionCleaner)\n  })\n  return wss\n}\n\nexport const sendMessage = (ws: WebSocket, server: ServerMessage) => {\n  ws.send(JSON.stringify(server))\n}\n\nexport function sendRequestReconnect() {\n  for (const ws of SWITCHBOARD.clients.keys()) {\n    sendMessage(ws, { type: 'action', data: { type: 'request-reconnect' } })\n  }\n}\n\nexport function waitForAllClientsDisconnected() {\n  return SWITCHBOARD.waitForAllClientsDisconnected()\n}\n",
            "tokens": 1495
        },
        "backend/src/websockets/websocket-action.ts": {
            "content": "import { calculateUsageAndBalance } from '@codebuff/billing'\nimport { ClientAction, ServerAction, UsageResponse } from 'common/actions'\nimport { toOptionalFile } from 'common/constants'\nimport { AnalyticsEvent } from 'common/constants/analytics-events'\nimport db from 'common/db'\nimport * as schema from 'common/db/schema'\nimport { trackEvent } from 'common/src/analytics'\nimport { ensureEndsWithNewline } from 'common/src/util/file'\nimport { buildArray } from 'common/util/array'\nimport { generateCompactId } from 'common/util/string'\nimport { ClientMessage } from 'common/websockets/websocket-schema'\nimport { eq } from 'drizzle-orm'\nimport { WebSocket } from 'ws'\n\nimport { mainPrompt } from '../main-prompt'\nimport { protec } from './middleware'\nimport { sendMessage } from './server'\n\nimport { logger, withLoggerContext } from '@/util/logger'\nimport { renderToolResults } from '@/util/parse-tool-call-xml'\n\n/**\n * Sends an action to the client via WebSocket\n * @param ws - The WebSocket connection to send the action to\n * @param action - The server action to send\n */\nexport const sendAction = (ws: WebSocket, action: ServerAction) => {\n  sendMessage(ws, {\n    type: 'action',\n    data: action,\n  })\n}\n\n/**\n * Retrieves a user ID from an authentication token\n * @param authToken - The authentication token to validate\n * @returns The user ID if found, undefined otherwise\n */\nexport const getUserIdFromAuthToken = async (\n  authToken?: string\n): Promise<string | undefined> => {\n  if (!authToken) return undefined\n\n  const userId = await db\n    .select({ userId: schema.user.id })\n    .from(schema.user)\n    .innerJoin(schema.session, eq(schema.user.id, schema.session.userId))\n    .where(eq(schema.session.sessionToken, authToken))\n    .then((users) => {\n      if (users.length === 1) {\n        return users[0].userId\n      }\n      return undefined\n    })\n\n  return userId\n}\n\n/**\n * Generates a usage response object for the client\n * @param fingerprintId - The fingerprint ID for the user/device\n * @param userId - user ID for authenticated users\n * @param clientSessionId - Optional session ID\n * @returns A UsageResponse object containing usage metrics and referral information\n */\nexport async function genUsageResponse(\n  fingerprintId: string,\n  userId: string,\n  clientSessionId: string | undefined\n): Promise<UsageResponse> {\n  const logContext = { fingerprintId, userId, sessionId: clientSessionId }\n  const defaultResp = {\n    type: 'usage-response' as const,\n    usage: 0,\n    remainingBalance: 0,\n    balanceBreakdown: {},\n    next_quota_reset: null,\n  }\n\n  return withLoggerContext(logContext, async () => {\n    const user = await db.query.user.findFirst({\n      where: eq(schema.user.id, userId),\n      columns: {\n        next_quota_reset: true,\n      },\n    })\n\n    if (!user) {\n      return defaultResp\n    }\n\n    try {\n      // Get the usage data\n      const { balance: balanceDetails, usageThisCycle } =\n        await calculateUsageAndBalance(userId, new Date())\n\n      return {\n        type: 'usage-response' as const,\n        usage: usageThisCycle,\n        remainingBalance: balanceDetails.totalRemaining,\n        balanceBreakdown: balanceDetails.breakdown,\n        next_quota_reset: user.next_quota_reset,\n      }\n    } catch (error) {\n      logger.error(\n        { error, usage: defaultResp },\n        'Error generating usage response, returning default'\n      )\n    }\n\n    return defaultResp\n  })\n}\n\n/**\n * Handles prompt actions from the client\n * @param action - The prompt action from the client\n * @param clientSessionId - The client's session ID\n * @param ws - The WebSocket connection\n */\nconst onPrompt = async (\n  action: Extract<ClientAction, { type: 'prompt' }>,\n  clientSessionId: string,\n  ws: WebSocket\n) => {\n  const { fingerprintId, authToken, promptId, prompt, toolResults, model } =\n    action\n\n  await withLoggerContext(\n    { fingerprintId, clientRequestId: promptId },\n    async () => {\n      if (prompt) logger.info(`USER INPUT: ${prompt}`)\n\n      const userId = await getUserIdFromAuthToken(authToken)\n      if (!userId) {\n        throw new Error('User not found')\n      }\n\n      trackEvent(AnalyticsEvent.PROMPT_SENT, userId, {\n        prompt,\n        promptId,\n      })\n\n      try {\n        const { agentState, toolCalls, toolResults } = await mainPrompt(\n          ws,\n          action,\n          userId,\n          clientSessionId,\n          (chunk) =>\n            sendAction(ws, {\n              type: 'response-chunk',\n              userInputId: promptId,\n              chunk,\n            }),\n          model\n        )\n\n        // Send prompt data back\n        sendAction(ws, {\n          type: 'prompt-response',\n          promptId,\n          agentState,\n          toolCalls,\n          toolResults,\n        })\n      } catch (e) {\n        logger.error(e, 'Error in mainPrompt')\n        let response =\n          e && typeof e === 'object' && 'message' in e ? `\\n\\n${e.message}` : ''\n        response += '\\n\\n<end_turn></end_turn>'\n\n        const newMessages = buildArray(\n          ...action.agentState.messageHistory,\n          prompt && {\n            role: 'user' as const,\n            content: prompt,\n          },\n          toolResults.length > 0 && {\n            role: 'user' as const,\n            content: renderToolResults(toolResults),\n          },\n          {\n            role: 'assistant' as const,\n            content: response,\n          }\n        )\n\n        const endTurnToolCall = {\n          name: 'end_turn' as const,\n          parameters: {},\n          id: generateCompactId(),\n        }\n\n        sendAction(ws, {\n          type: 'response-chunk',\n          userInputId: promptId,\n          chunk: response,\n        })\n        setTimeout(() => {\n          sendAction(ws, {\n            type: 'prompt-response',\n            promptId,\n            // Send back original agentState.\n            agentState: {\n              ...action.agentState,\n              messageHistory: newMessages,\n            },\n            toolCalls: [endTurnToolCall],\n            toolResults: [],\n          })\n        }, 100)\n      } finally {\n        const usageResponse = await genUsageResponse(\n          fingerprintId,\n          userId,\n          undefined\n        )\n        sendAction(ws, usageResponse)\n      }\n    }\n  )\n}\n\n/**\n * Handles initialization actions from the client\n * @param fileContext - The file context information\n * @param fingerprintId - The fingerprint ID for the user/device\n * @param authToken - The authentication token\n * @param clientSessionId - The client's session ID\n * @param ws - The WebSocket connection\n */\nconst onInit = async (\n  {\n    fileContext,\n    fingerprintId,\n    authToken,\n  }: Extract<ClientAction, { type: 'init' }>,\n  clientSessionId: string,\n  ws: WebSocket\n) => {\n  await withLoggerContext({ fingerprintId }, async () => {\n    const userId = await getUserIdFromAuthToken(authToken)\n\n    if (!userId) {\n      sendAction(ws, {\n        usage: 0,\n        remainingBalance: 0,\n        balanceBreakdown: {},\n        next_quota_reset: null,\n        type: 'init-response',\n      })\n      return\n    }\n\n    // Send combined init and usage response\n    const usageResponse = await genUsageResponse(\n      fingerprintId,\n      userId,\n      clientSessionId\n    )\n    sendAction(ws, {\n      ...usageResponse,\n      type: 'init-response',\n    })\n  })\n}\n\n/**\n * Storage for action callbacks organized by action type\n */\nconst callbacksByAction = {} as Record<\n  ClientAction['type'],\n  ((action: ClientAction, clientSessionId: string, ws: WebSocket) => void)[]\n>\n\n/**\n * Subscribes a callback function to a specific action type\n * @param type - The action type to subscribe to\n * @param callback - The callback function to execute when the action is received\n * @returns A function to unsubscribe the callback\n */\nexport const subscribeToAction = <T extends ClientAction['type']>(\n  type: T,\n  callback: (\n    action: Extract<ClientAction, { type: T }>,\n    clientSessionId: string,\n    ws: WebSocket\n  ) => void\n) => {\n  callbacksByAction[type] = (callbacksByAction[type] ?? []).concat(\n    callback as (\n      action: ClientAction,\n      clientSessionId: string,\n      ws: WebSocket\n    ) => void\n  )\n  return () => {\n    callbacksByAction[type] = (callbacksByAction[type] ?? []).filter(\n      (cb) => cb !== callback\n    )\n  }\n}\n\n/**\n * Handles WebSocket action messages from clients\n * @param ws - The WebSocket connection\n * @param clientSessionId - The client's session ID\n * @param msg - The action message from the client\n */\nexport const onWebsocketAction = async (\n  ws: WebSocket,\n  clientSessionId: string,\n  msg: ClientMessage & { type: 'action' }\n) => {\n  await withLoggerContext({ clientSessionId }, async () => {\n    const callbacks = callbacksByAction[msg.data.type] ?? []\n    try {\n      await Promise.all(\n        callbacks.map((cb) => cb(msg.data, clientSessionId, ws))\n      )\n    } catch (e) {\n      logger.error(\n        {\n          message: msg,\n          error: e && typeof e === 'object' && 'message' in e ? e.message : e,\n        },\n        'Got error running subscribeToAction callback'\n      )\n    }\n  })\n}\n\n// Register action handlers\nsubscribeToAction('prompt', protec.run(onPrompt))\nsubscribeToAction('init', protec.run(onInit, { silent: true }))\n\n/**\n * Requests multiple files from the client\n * @param ws - The WebSocket connection\n * @param filePaths - Array of file paths to request\n * @returns Promise resolving to an object mapping file paths to their contents\n */\nexport async function requestFiles(ws: WebSocket, filePaths: string[]) {\n  return new Promise<Record<string, string | null>>((resolve) => {\n    const requestId = generateCompactId()\n    const unsubscribe = subscribeToAction('read-files-response', (action) => {\n      for (const [filename, contents] of Object.entries(action.files)) {\n        action.files[filename] = ensureEndsWithNewline(contents)\n      }\n      if (action.requestId === requestId) {\n        unsubscribe()\n        resolve(action.files)\n      }\n    })\n    sendAction(ws, {\n      type: 'read-files',\n      filePaths,\n      requestId,\n    })\n  })\n}\n\n/**\n * Requests a single file from the client\n * @param ws - The WebSocket connection\n * @param filePath - The path of the file to request\n * @returns Promise resolving to the file contents or null if not found\n */\nexport async function requestFile(ws: WebSocket, filePath: string) {\n  const files = await requestFiles(ws, [filePath])\n  return files[filePath] ?? null\n}\n\nexport async function requestOptionalFile(ws: WebSocket, filePath: string) {\n  const file = await requestFile(ws, filePath)\n  return toOptionalFile(file)\n}\n",
            "tokens": 3362
        },
        "common/src/actions.ts": {
            "content": "import { z } from 'zod'\n\nimport { costModes } from './constants'\nimport {\n  AgentStateSchema,\n  ToolCallSchema as NewToolCallSchema,\n  ToolResultSchema,\n} from './types/agent-state'\nimport { GrantTypeValues } from './types/grant'\nimport { FileVersionSchema, ProjectFileContextSchema } from './util/file'\n\nexport const FileChangeSchema = z.object({\n  type: z.enum(['patch', 'file']),\n  path: z.string(),\n  content: z.string(),\n})\nexport type FileChange = z.infer<typeof FileChangeSchema>\nexport const CHANGES = z.array(FileChangeSchema)\nexport type FileChanges = z.infer<typeof CHANGES>\n\nexport const ToolCallSchema = z.object({\n  name: z.string(),\n  id: z.string(),\n  input: z.record(z.string(), z.any()),\n})\nexport type ToolCall = z.infer<typeof ToolCallSchema>\n\nexport const CLIENT_ACTION_SCHEMA = z.discriminatedUnion('type', [\n  z.object({\n    type: z.literal('prompt'),\n    promptId: z.string(),\n    prompt: z.string().or(z.undefined()),\n    fingerprintId: z.string(),\n    authToken: z.string().optional(),\n    costMode: z.enum(costModes).optional().default('normal'),\n    agentState: AgentStateSchema,\n    toolResults: z.array(ToolResultSchema),\n    model: z.string().optional(),\n    cwd: z.string().optional(),\n  }),\n  z.object({\n    type: z.literal('read-files-response'),\n    files: z.record(z.string(), z.union([z.string(), z.null()])),\n    requestId: z.string().optional(),\n  }),\n  z.object({\n    type: z.literal('init'),\n    fingerprintId: z.string(),\n    authToken: z.string().optional(),\n    fileContext: ProjectFileContextSchema,\n  }),\n  z.object({\n    type: z.literal('generate-commit-message'),\n    fingerprintId: z.string(),\n    authToken: z.string().optional(),\n    stagedChanges: z.string(),\n  }),\n])\nexport type ClientAction = z.infer<typeof CLIENT_ACTION_SCHEMA>\n\nexport const UsageReponseSchema = z.object({\n  type: z.literal('usage-response'),\n  usage: z.number(),\n  remainingBalance: z.number(),\n  balanceBreakdown: z\n    .record(\n      z.enum([GrantTypeValues[0], ...GrantTypeValues.slice(1)]),\n      z.number()\n    )\n    .optional(),\n  next_quota_reset: z.coerce.date().nullable(),\n  autoTopupAdded: z.number().optional(),\n})\nexport type UsageResponse = z.infer<typeof UsageReponseSchema>\n\nexport const InitResponseSchema = z\n  .object({\n    type: z.literal('init-response'),\n  })\n  .merge(\n    UsageReponseSchema.omit({\n      type: true,\n    })\n  )\nexport type InitResponse = z.infer<typeof InitResponseSchema>\n\nexport const ResponseCompleteSchema = z\n  .object({\n    type: z.literal('response-complete'),\n    userInputId: z.string(),\n    response: z.string(),\n    changes: CHANGES,\n    changesAlreadyApplied: CHANGES,\n    addedFileVersions: z.array(FileVersionSchema),\n    resetFileVersions: z.boolean(),\n  })\n  .merge(\n    UsageReponseSchema.omit({\n      type: true,\n    }).partial()\n  )\n\nexport const MessageCostResponseSchema = z.object({\n  type: z.literal('message-cost-response'),\n  promptId: z.string(),\n  credits: z.number(),\n})\nexport type MessageCostResponse = z.infer<typeof MessageCostResponseSchema>\n\nexport const PromptResponseSchema = z.object({\n  type: z.literal('prompt-response'),\n  promptId: z.string(),\n  agentState: AgentStateSchema,\n  toolCalls: z.array(NewToolCallSchema),\n  toolResults: z.array(ToolResultSchema),\n})\nexport type PromptResponse = z.infer<typeof PromptResponseSchema>\n\nexport const SERVER_ACTION_SCHEMA = z.discriminatedUnion('type', [\n  z.object({\n    type: z.literal('response-chunk'),\n    userInputId: z.string(),\n    chunk: z.string(),\n  }),\n  ResponseCompleteSchema,\n  PromptResponseSchema,\n  z.object({\n    type: z.literal('read-files'),\n    filePaths: z.array(z.string()),\n    requestId: z.string(),\n  }),\n  z.object({\n    type: z.literal('tool-call'),\n    userInputId: z.string(),\n    response: z.string(),\n    data: ToolCallSchema,\n    changes: CHANGES,\n    changesAlreadyApplied: CHANGES,\n    addedFileVersions: z.array(FileVersionSchema),\n    resetFileVersions: z.boolean(),\n  }),\n  z.object({\n    type: z.literal('terminal-command-result'),\n    userInputId: z.string(),\n    result: z.string(),\n  }),\n  z.object({\n    type: z.literal('npm-version-status'),\n    isUpToDate: z.boolean(),\n    latestVersion: z.string(),\n  }),\n  InitResponseSchema,\n  UsageReponseSchema,\n  MessageCostResponseSchema,\n  z.object({\n    type: z.literal('action-error'),\n    message: z.string(),\n    error: z.string().optional(),\n    remainingBalance: z.number().optional(),\n  }),\n  z.object({\n    type: z.literal('commit-message-response'),\n    commitMessage: z.string(),\n  }),\n  z.object({\n    // The server is imminently going to shutdown, and the client should reconnect\n    type: z.literal('request-reconnect'),\n  }),\n])\n\nexport type ServerAction = z.infer<typeof SERVER_ACTION_SCHEMA>\n",
            "tokens": 1567
        },
        "common/src/constants.ts": {
            "content": "export const STOP_MARKER = '[' + 'END]'\nexport const FIND_FILES_MARKER = '[' + 'FIND_FILES_PLEASE]'\nexport const EXISTING_CODE_MARKER = '[[**REPLACE_WITH_EXISTING_CODE**]]'\n\nexport const DEFAULT_IGNORED_FILES = [\n  '.git',\n  '.env',\n  '.env.*',\n  'env',\n  'ENV',\n  '*.min.*',\n  'node_modules',\n  'venv',\n  'virtualenv',\n  '.venv',\n  '.virtualenv',\n  '__pycache__',\n  '*.egg-info/',\n  '*.pyc',\n  '.DS_Store',\n  '.pytest_cache',\n  '.mypy_cache',\n  '.ruff_cache',\n  '.next',\n  'package-lock.json',\n  'bun.lockb',\n]\n\n// Special message content tags indicating specific server states\nexport const ASKED_CONFIG = 'asked_config'\nexport const SHOULD_ASK_CONFIG = 'should_ask_config'\nexport const ONE_TIME_TAGS = [] as const\nexport const ONE_TIME_LABELS = [\n  ...ONE_TIME_TAGS,\n  ASKED_CONFIG,\n  SHOULD_ASK_CONFIG,\n] as const\n\nexport const FILE_READ_STATUS = {\n  DOES_NOT_EXIST: '[FILE_DOES_NOT_EXIST]',\n  IGNORED: '[FILE_IGNORED_BY_GITIGNORE_OR_CODEBUFF_IGNORE]',\n  OUTSIDE_PROJECT: '[FILE_OUTSIDE_PROJECT]',\n  TOO_LARGE: '[FILE_TOO_LARGE]',\n  ERROR: '[FILE_READ_ERROR]',\n} as const\n\nexport const HIDDEN_FILE_READ_STATUS = [\n  FILE_READ_STATUS.DOES_NOT_EXIST,\n  FILE_READ_STATUS.IGNORED,\n  FILE_READ_STATUS.OUTSIDE_PROJECT,\n  FILE_READ_STATUS.TOO_LARGE,\n  FILE_READ_STATUS.ERROR,\n]\n\nexport function toOptionalFile(file: string | null) {\n  if (file === null) return null\n  return HIDDEN_FILE_READ_STATUS.some((status) => file.startsWith(status))\n    ? null\n    : file\n}\n\nexport const REQUEST_CREDIT_SHOW_THRESHOLD = 1\nexport const MAX_DATE = new Date(86399999999999)\nexport const BILLING_PERIOD_DAYS = 30\nexport const CREDITS_REFERRAL_BONUS = 250\nexport const AFFILIATE_USER_REFFERAL_LIMIT = 500\n\n// Default number of free credits granted per cycle\nexport const DEFAULT_FREE_CREDITS_GRANT = 500\n\nexport const AuthState = {\n  LOGGED_OUT: 'LOGGED_OUT',\n  LOGGED_IN: 'LOGGED_IN',\n} as const\n\nexport type AuthState = (typeof AuthState)[keyof typeof AuthState]\n\nexport const UserState = {\n  LOGGED_OUT: 'LOGGED_OUT',\n  GOOD_STANDING: 'GOOD_STANDING', // >= 100 credits\n  ATTENTION_NEEDED: 'ATTENTION_NEEDED', // 20-99 credits\n  CRITICAL: 'CRITICAL', // 1-19 credits\n  DEPLETED: 'DEPLETED', // <= 0 credits\n} as const\n\nexport type UserState = (typeof UserState)[keyof typeof UserState]\n\nexport function getUserState(isLoggedIn: boolean, credits: number): UserState {\n  if (!isLoggedIn) return UserState.LOGGED_OUT\n\n  if (credits >= 100) return UserState.GOOD_STANDING\n  if (credits >= 20) return UserState.ATTENTION_NEEDED\n  if (credits >= 1) return UserState.CRITICAL\n  return UserState.DEPLETED\n}\n\nexport const costModes = ['lite', 'normal', 'max', 'experimental'] as const\nexport type CostMode = (typeof costModes)[number]\n\nexport const getModelForMode = (\n  costMode: CostMode,\n  operation: 'agent' | 'file-requests' | 'check-new-files'\n) => {\n  if (operation === 'agent') {\n    return costMode === 'experimental'\n      ? models.gpt4_1\n      : costMode === 'lite'\n        ? models.gemini2_5_flash_thinking\n        : claudeModels.sonnet\n  }\n  if (operation === 'file-requests') {\n    return costMode === 'max' ? claudeModels.sonnet : claudeModels.haiku\n  }\n  if (operation === 'check-new-files') {\n    return costMode === 'lite' ? models.gpt4omini : models.gpt4o\n  }\n  throw new Error(`Unknown operation: ${operation}`)\n}\n\nexport const claudeModels = {\n  sonnet: 'claude-3-5-sonnet-20241022',\n  sonnet3_7: 'claude-3-7-sonnet-20250219',\n  haiku: 'claude-3-5-haiku-20241022',\n} as const\nexport type AnthropicModel = (typeof claudeModels)[keyof typeof claudeModels]\n\nexport const openaiModels = {\n  gpt4_1: 'gpt-4.1-2025-04-14',\n  gpt4o: 'gpt-4o-2024-11-20',\n  gpt4omini: 'gpt-4o-mini-2024-07-18',\n  o3mini: 'o3-mini-2025-01-31',\n  o3: 'o3-2025-04-16',\n  o4mini: 'o4-mini-2025-04-16',\n  generatePatch:\n    'ft:gpt-4o-2024-08-06:manifold-markets:generate-patch-batch2:AKYtDIhk',\n} as const\nexport type OpenAIModel = (typeof openaiModels)[keyof typeof openaiModels]\n\nexport const geminiModels = {\n  gemini2_5_flash: 'gemini-2.5-flash-preview-04-17',\n  gemini2_5_flash_thinking: 'gemini-2.5-flash-preview-04-17:thinking',\n  gemini2flash: 'gemini-2.0-flash-001',\n  gemini2_5_pro_exp: 'gemini-2.5-pro-exp-03-25',\n  gemini2_5_pro_preview: 'gemini-2.5-pro-preview-03-25',\n} as const\nexport type GeminiModel = (typeof geminiModels)[keyof typeof geminiModels]\n\nexport const openrouterModels = {\n  openrouter_gemini2_5_pro_exp: 'google/gemini-2.5-pro-exp-03-25:free',\n  openrouter_gemini2_5_pro_preview: 'google/gemini-2.5-pro-preview-03-25',\n  openrouter_gemini2_5_flash: 'google/gemini-2.5-flash-preview',\n  openrouter_gemini2_5_flash_thinking:\n    'google/gemini-2.5-flash-preview:thinking',\n} as const\nexport type openrouterModel =\n  (typeof openrouterModels)[keyof typeof openrouterModels]\n\nexport const deepseekModels = {\n  deepseekChat: 'deepseek-chat',\n  deepseekReasoner: 'deepseek-reasoner',\n} as const\nexport type DeepseekModel = (typeof deepseekModels)[keyof typeof deepseekModels]\n\n// Vertex uses \"endpoint IDs\" for finetuned models, which are just integers\nexport const finetunedVertexModels = {\n  ft_filepicker_003: '196166068534771712',\n  ft_filepicker_005: '8493203957034778624',\n} as const\nexport type FinetunedVertexModel =\n  (typeof finetunedVertexModels)[keyof typeof finetunedVertexModels]\n\nexport const models = {\n  ...claudeModels,\n  ...openaiModels,\n  ...geminiModels,\n  ...deepseekModels,\n  ...openrouterModels,\n  ...finetunedVertexModels,\n} as const\n\nexport const shortModelNames = {\n  'gemini-2.5-pro': models.gemini2_5_pro_preview,\n  'sonnet-3.7': models.sonnet3_7,\n  'sonnet-3.5': models.sonnet,\n  'sonnet-3.6': models.sonnet,\n  'gpt-4.1': models.gpt4_1,\n  'o3-mini': models.o3mini,\n  o3: models.o3,\n  'o4-mini': models.o4mini,\n}\n\nexport const providerModelNames = {\n  [models.gemini2_5_pro_preview]: 'gemini',\n  [models.gemini2_5_pro_exp]: 'gemini',\n  [models.gemini2flash]: 'gemini',\n  [models.gemini2_5_flash]: 'gemini',\n  [models.gemini2_5_flash_thinking]: 'gemini',\n  [models.haiku]: 'anthropic',\n  [models.sonnet3_7]: 'anthropic',\n  [models.sonnet]: 'anthropic',\n  [models.gpt4_1]: 'openai',\n  [models.gpt4o]: 'openai',\n  [models.gpt4omini]: 'openai',\n  [models.o3mini]: 'openai',\n  [models.o3]: 'openai',\n  [models.o4mini]: 'openai',\n}\n\nexport type Model = (typeof models)[keyof typeof models]\n\nexport const TEST_USER_ID = 'test-user-id'\n",
            "tokens": 2718
        },
        "common/src/types/message.ts": {
            "content": "import { z } from 'zod'\n\nconst MessageContentObjectSchema = z.union([\n  z.object({\n    type: z.literal('text'),\n    text: z.string(),\n    cache_control: z\n      .object({\n        type: z.literal('ephemeral'),\n      })\n      .optional(),\n  }),\n  z.object({\n    type: z.literal('tool_use'),\n    id: z.string(),\n    name: z.string(),\n    input: z.record(z.string(), z.any()),\n    cache_control: z\n      .object({\n        type: z.literal('ephemeral'),\n      })\n      .optional(),\n  }),\n  z.object({\n    type: z.literal('tool_result'),\n    tool_use_id: z.string(),\n    content: z.string(),\n    cache_control: z\n      .object({\n        type: z.literal('ephemeral'),\n      })\n      .optional(),\n  }),\n  z.object({\n    type: z.literal('image'),\n    source: z.object({\n      type: z.literal('base64'),\n      media_type: z.literal('image/jpeg'),\n      data: z.string(),\n    }),\n    cache_control: z\n      .object({\n        type: z.literal('ephemeral'),\n      })\n      .optional(),\n  }),\n])\n\nexport const MessageSchema = z.object({\n  role: z.union([z.literal('user'), z.literal('assistant')]),\n  content: z.union([z.string(), z.array(MessageContentObjectSchema)]),\n})\nexport type Message = z.infer<typeof MessageSchema>\nexport type MessageContentObject = z.infer<typeof MessageContentObjectSchema>\n",
            "tokens": 434
        },
        "common/src/util/file.ts": {
            "content": "import * as fs from 'fs'\nimport { z } from 'zod'\n\nexport const FileTreeNodeSchema: z.ZodType<FileTreeNode> = z.object({\n  name: z.string(),\n  type: z.enum(['file', 'directory']),\n  children: z.lazy(() => z.array(FileTreeNodeSchema).optional()),\n  filePath: z.string(),\n})\n\nexport interface FileTreeNode {\n  name: string\n  type: 'file' | 'directory'\n  filePath: string\n  lastReadTime?: number\n  children?: FileTreeNode[]\n}\n\nexport interface DirectoryNode extends FileTreeNode {\n  type: 'directory'\n  children: FileTreeNode[]\n}\n\nexport interface FileNode extends FileTreeNode {\n  type: 'file'\n  lastReadTime: number\n}\n\nexport const FileVersionSchema = z.object({\n  path: z.string(),\n  content: z.string(),\n})\n\nexport type FileVersion = z.infer<typeof FileVersionSchema>\n\nexport const ProjectFileContextSchema = z.object({\n  currentWorkingDirectory: z.string(),\n  fileTree: z.array(z.custom<FileTreeNode>()),\n  fileTokenScores: z.record(z.string(), z.record(z.string(), z.number())),\n  knowledgeFiles: z.record(z.string(), z.string()),\n  userKnowledgeFiles: z.record(z.string(), z.string()).optional(),\n  gitChanges: z.object({\n    status: z.string(),\n    diff: z.string(),\n    diffCached: z.string(),\n    lastCommitMessages: z.string(),\n  }),\n  changesSinceLastChat: z.record(z.string(), z.string()),\n  shellConfigFiles: z.record(z.string(), z.string()),\n  systemInfo: z.object({\n    platform: z.string(),\n    shell: z.string(),\n    nodeVersion: z.string(),\n    arch: z.string(),\n    homedir: z.string(),\n    cpus: z.number(),\n  }),\n  fileVersions: z.array(z.array(FileVersionSchema)).optional(), // Keep temporarily for migration\n})\n\nexport type ProjectFileContext = z.infer<typeof ProjectFileContextSchema>\n\nexport const createWriteFileBlock = (filePath: string, content: string) => {\n  const tagName = 'write_file'\n  return (\n    '<' +\n    `${tagName}>\n<path>${filePath}</path>\n<content>\n${content}\n</content>\n</${tagName}>`\n  )\n}\n\nexport const fileRegex =\n  /<write_file>\\s*<path>([^<]+)<\\/path>\\s*<content>([\\s\\S]*?)<\\/content>\\s*<\\/write_file>/g\nexport const fileWithNoPathRegex = /<write_file>([\\s\\S]*?)<\\/write_file>/g\n\nexport const parseFileBlocks = (fileBlocks: string) => {\n  let fileMatch\n  const files: Record<string, string> = {}\n  while ((fileMatch = fileRegex.exec(fileBlocks)) !== null) {\n    const [, filePath, fileContent] = fileMatch\n    files[filePath] = fileContent.startsWith('\\n')\n      ? fileContent.slice(1)\n      : fileContent\n  }\n  return files\n}\n\nexport const createMarkdownFileBlock = (filePath: string, content: string) => {\n  return `\\`\\`\\`${filePath}\\n${content}\\n\\`\\`\\``\n}\n\nexport const parseMarkdownCodeBlock = (content: string) => {\n  const match = content.match(/^```(?:[a-zA-Z]+)?\\n([\\s\\S]*)\\n```$/)\n  if (match) {\n    return match[1] + '\\n'\n  }\n  return content\n}\n\nexport const createSearchReplaceBlock = (search: string, replace: string) => {\n  return `<<<<<<< SEARCH\\n${search}\\n=======\\n${replace}\\n>>>>>>> REPLACE`\n}\n\nexport function printFileTree(\n  nodes: FileTreeNode[],\n  depth: number = 0\n): string {\n  let result = ''\n  const indentation = ' '.repeat(depth)\n  for (const node of nodes) {\n    result += `${indentation}${node.name}${node.type === 'directory' ? '/' : ''}\\n`\n    if (node.type === 'directory' && node.children) {\n      result += printFileTree(node.children, depth + 1)\n    }\n  }\n  return result\n}\n\nexport function printFileTreeWithTokens(\n  nodes: FileTreeNode[],\n  fileTokenScores: Record<string, Record<string, number>>,\n  path: string[] = []\n): string {\n  let result = ''\n  const depth = path.length\n  const indentToken = ' '\n  const indentation = indentToken.repeat(depth)\n  const indentationWithFile = indentToken.repeat(depth + 1)\n  for (const node of nodes) {\n    if (\n      node.type === 'directory' &&\n      (!node.children || node.children.length === 0)\n    ) {\n      // Skip empty directories\n      continue\n    }\n    result += `${indentation}${node.name}${node.type === 'directory' ? '/' : ''}`\n    path.push(node.name)\n    const filePath = path.join('/')\n    const tokenScores = fileTokenScores[filePath]\n    if (node.type === 'file' && tokenScores) {\n      const tokens = Object.keys(tokenScores)\n      if (tokens.length > 0) {\n        result += `\\n${indentationWithFile}${tokens.join(' ')}`\n      }\n    }\n    result += '\\n'\n    if (node.type === 'directory' && node.children) {\n      result += printFileTreeWithTokens(node.children, fileTokenScores, path)\n    }\n    path.pop()\n  }\n  return result\n}\n\n/**\n * Ensures the given file contents ends with a newline character.\n * @param contents - The file contents\n * @returns the file contents with a newline character.\n */\nexport const ensureEndsWithNewline = (\n  contents: string | null\n): string | null => {\n  if (contents === null || contents === '') {\n    // Leave empty file as is\n    return contents\n  }\n  if (contents.endsWith('\\n')) {\n    return contents\n  }\n  return contents + '\\n'\n}\n\nexport const ensureDirectoryExists = (baseDir: string) => {\n  if (!fs.existsSync(baseDir)) {\n    fs.mkdirSync(baseDir, { recursive: true })\n  }\n}\n\n/**\n * Removes markdown code block syntax if present, including any language tag\n */\nexport const cleanMarkdownCodeBlock = (content: string): string => {\n  const cleanResponse = content.match(/^```(?:[a-zA-Z]+)?\\n([\\s\\S]*)\\n```$/)\n    ? content.replace(/^```(?:[a-zA-Z]+)?\\n/, '').replace(/\\n```$/, '')\n    : content\n  return cleanResponse\n}\n\nexport function isValidFilePath(path: string) {\n  if (!path) return false\n\n  // Check for whitespace\n  if (/\\s/.test(path)) return false\n\n  // Check for invalid characters\n  const invalidChars = /[<>:\"|?*\\x00-\\x1F]/g\n  if (invalidChars.test(path)) return false\n\n  return true\n}\n",
            "tokens": 2019
        },
        "common/src/util/logger.ts": {
            "content": "import type { AsyncLocalStorage as NodeAsyncLocalStorage } from 'async_hooks'\nimport path from 'path'\n\nimport pino from 'pino'\n\nimport { env } from '../env.mjs'\n\nlet AsyncLocalStorageImpl: typeof import('async_hooks').AsyncLocalStorage | null\ntry {\n  // Load AsyncLocalStorage via require\n  AsyncLocalStorageImpl = require('async_hooks').AsyncLocalStorage\n} catch {\n  AsyncLocalStorageImpl = null\n}\n\n// Create a no‑op shim when AsyncLocalStorage isn't present\nconst loggerAsyncStorage =\n  AsyncLocalStorageImpl !== null\n    ? new AsyncLocalStorageImpl<LoggerContext>()\n    : {\n        // run() just executes fn without context tracking\n        run: <R, A extends any[]>(_: any, fn: (...args: A) => R, ...args: A) =>\n          fn(...args),\n        getStore: () => undefined,\n      }\n\nexport interface LoggerContext {\n  userId?: string\n  userEmail?: string\n  clientSessionId?: string\n  [key: string]: any // Allow for future extensions\n}\n\nexport const withLoggerContext = <T>(\n  additionalContext: Partial<LoggerContext>,\n  fn: () => Promise<T>\n) => {\n  const store = (loggerAsyncStorage.getStore?.() ?? {}) as LoggerContext\n  // Cast to Node's AsyncLocalStorage to resolve overload mismatch\n  return (loggerAsyncStorage as NodeAsyncLocalStorage<LoggerContext>).run(\n    { ...store, ...additionalContext },\n    fn\n  )\n}\n\n// Only use file transport when not running in Edge/browser‑like env\nconst runningInEdge = process.env.NEXT_RUNTIME === 'edge'\nconst fileTransport =\n  !runningInEdge && env.NEXT_PUBLIC_CB_ENVIRONMENT !== 'production'\n    ? pino.transport({\n        target: 'pino/file',\n        options: { destination: path.join(__dirname, '..', 'debug.log') },\n        level: 'debug',\n      })\n    : undefined\n\nexport const logger = pino(\n  {\n    level: 'debug',\n    mixin() {\n      // If AsyncLocalStorage isn't available, return undefined\n      return { logTrace: loggerAsyncStorage.getStore?.() }\n    },\n    formatters: {\n      level: (label) => ({ level: label.toUpperCase() }),\n    },\n    timestamp: () => `,\"timestamp\":\"${new Date().toISOString()}\"`,\n  },\n  fileTransport\n)\n",
            "tokens": 696
        },
        "common/src/util/messages.ts": {
            "content": "import { Message } from '../types/message'\n\ninterface ScreenshotRef {\n  msgIdx: number\n  contentIdx: number\n}\n\n/**\n * Limits the total number of screenshots across all messages to maxCount,\n * keeping only the most recent ones.\n */\nexport function limitScreenshots(\n  messages: Message[],\n  maxCount: number\n): Message[] {\n  const screenshots = messages.flatMap((msg, msgIdx) =>\n    Array.isArray(msg.content)\n      ? msg.content\n          .map((item, contentIdx) =>\n            item.type === 'image' ? { msgIdx, contentIdx } : null\n          )\n          .filter((ref): ref is ScreenshotRef => ref !== null)\n      : []\n  )\n\n  if (screenshots.length <= maxCount) return messages\n\n  const keepRefs = new Set(\n    screenshots.slice(-maxCount).map((ref) => `${ref.msgIdx}-${ref.contentIdx}`)\n  )\n\n  return messages.map((msg, msgIdx) =>\n    Array.isArray(msg.content)\n      ? {\n          ...msg,\n          content: msg.content.filter(\n            (item, contentIdx) =>\n              item.type !== 'image' || keepRefs.has(`${msgIdx}-${contentIdx}`)\n          ),\n        }\n      : msg\n  )\n}\n\nexport function toContentString(msg: Message): string {\n  const { content } = msg\n  if (typeof content === 'string') return content\n  return content.map((item) => (item as any)?.text ?? '').join('\\n')\n}\n\nexport function withCacheControl(msg: Message): Message {\n  if (typeof msg.content === 'string') {\n    return {\n      ...msg,\n      content: [\n        {\n          type: 'text',\n          text: msg.content,\n          cache_control: { type: 'ephemeral' as const },\n        },\n      ],\n    }\n  } else {\n    return {\n      ...msg,\n      content: msg.content.map((item, i) =>\n        i === msg.content.length - 1\n          ? { ...item, cache_control: { type: 'ephemeral' as const } }\n          : item\n      ),\n    }\n  }\n}\n\nexport function removeCache(messages: Message[]): Message[] {\n  return messages.map(msg => {\n    if (typeof msg.content === 'object' && Array.isArray(msg.content)) {\n      return {\n        ...msg,\n        content: msg.content.map(item => {\n          const { cache_control, ...rest } = item\n          return rest\n        })\n      }\n    }\n    return msg\n  })\n}\n",
            "tokens": 707
        },
        "common/src/util/object.ts": {
            "content": "import { isEqual, mapValues, union } from 'lodash'\n\nexport const removeUndefinedProps = <T extends object>(obj: T): T => {\n  const newObj: any = {}\n\n  for (const key of Object.keys(obj)) {\n    if ((obj as any)[key] !== undefined) newObj[key] = (obj as any)[key]\n  }\n\n  return newObj\n}\n\nexport const removeNullOrUndefinedProps = <T extends object>(\n  obj: T,\n  exceptions?: string[]\n): T => {\n  const newObj: any = {}\n\n  for (const key of Object.keys(obj)) {\n    if (\n      ((obj as any)[key] !== undefined && (obj as any)[key] !== null) ||\n      (exceptions ?? []).includes(key)\n    )\n      newObj[key] = (obj as any)[key]\n  }\n  return newObj\n}\n\nexport const addObjects = <T extends { [key: string]: number }>(\n  obj1: T,\n  obj2: T\n) => {\n  const keys = union(Object.keys(obj1), Object.keys(obj2))\n  const newObj = {} as any\n\n  for (const key of keys) {\n    newObj[key] = (obj1[key] ?? 0) + (obj2[key] ?? 0)\n  }\n\n  return newObj as T\n}\n\nexport const subtractObjects = <T extends { [key: string]: number }>(\n  obj1: T,\n  obj2: T\n) => {\n  const keys = union(Object.keys(obj1), Object.keys(obj2))\n  const newObj = {} as any\n\n  for (const key of keys) {\n    newObj[key] = (obj1[key] ?? 0) - (obj2[key] ?? 0)\n  }\n\n  return newObj as T\n}\n\nexport const hasChanges = <T extends object>(obj: T, partial: Partial<T>) => {\n  const currValues = mapValues(partial, (_, key: keyof T) => obj[key])\n  return !isEqual(currValues, partial)\n}\n\nexport const hasSignificantDeepChanges = <T extends object>(\n  obj: T,\n  partial: Partial<T>,\n  epsilonForNumbers: number\n): boolean => {\n  const compareValues = (currValue: any, partialValue: any): boolean => {\n    if (typeof currValue === 'number' && typeof partialValue === 'number') {\n      return Math.abs(currValue - partialValue) > epsilonForNumbers\n    }\n    if (typeof currValue === 'object' && typeof partialValue === 'object') {\n      return hasSignificantDeepChanges(\n        currValue,\n        partialValue,\n        epsilonForNumbers\n      )\n    }\n    return !isEqual(currValue, partialValue)\n  }\n\n  for (const key in partial) {\n    if (Object.prototype.hasOwnProperty.call(partial, key)) {\n      if (compareValues(obj[key], partial[key])) {\n        return true\n      }\n    }\n  }\n\n  return false\n}\n\nexport const filterObject = <T extends object>(obj: T, predicate: (value: any, key: keyof T) => boolean): { [P in keyof T]: T[P] } => {\n  const result = {} as { [P in keyof T]: T[P] }\n  for (const key in obj) {\n    if (Object.prototype.hasOwnProperty.call(obj, key)) {\n      if (predicate(obj[key], key)) {\n        result[key] = obj[key]\n      }\n    }\n  }\n  return result\n}\n\n/**\n * Asserts that a condition is true. If the condition is false, it throws an error with the provided message.\n * @param condition The condition to check\n * @param message The error message to display if the condition is false\n * @throws {Error} If the condition is false\n */\nexport function assert(condition: boolean, message: string): asserts condition {\n  if (!condition) {\n    throw new Error(`Assertion failed: ${message}`);\n  }\n}\n",
            "tokens": 1136
        },
        "common/src/util/saxy.ts": {
            "content": "/**\n * This is a modified version of the Saxy library that emits text nodes immediately\n */\nimport { StringDecoder } from 'string_decoder'\n\nimport { Transform } from 'readable-stream'\n\nimport { isWhitespace } from './string'\n\nexport type TextNode = {\n  /** The text value */\n  contents: string\n}\n\nexport type CDATANode = {\n  /** The CDATA contents */\n  contents: string\n}\n\nexport type CommentNode = {\n  /** The comment contents */\n  contents: string\n}\n\nexport type ProcessingInstructionNode = {\n  /** The instruction contents */\n  contents: string\n}\n\n/** Information about an opened tag */\nexport type TagOpenNode = {\n  /** Name of the tag that was opened. */\n  name: string\n  /**\n   * Attributes passed to the tag, in a string representation\n   * (use Saxy.parseAttributes to get an attribute-value mapping).\n   */\n  attrs: string\n  /**\n   * Whether the tag self-closes (tags of the form `<tag />`).\n   * Such tags will not be followed by a closing tag.\n   */\n  isSelfClosing: boolean\n}\n\n/** Information about a closed tag */\nexport type TagCloseNode = {\n  /** Name of the tag that was closed. */\n  name: string\n}\n\nexport type NextFunction = (err?: Error) => void\n\nexport interface SaxyEvents {\n  finish: () => void\n  error: (err: Error) => void\n  text: (data: TextNode) => void\n  cdata: (data: CDATANode) => void\n  comment: (data: CommentNode) => void\n  processinginstruction: (data: ProcessingInstructionNode) => void\n  tagopen: (data: TagOpenNode) => void\n  tagclose: (data: TagCloseNode) => void\n}\n\nexport type SaxyEventNames = keyof SaxyEvents\n\nexport type SaxyEventArgs =\n  | Error\n  | TextNode\n  | CDATANode\n  | CommentNode\n  | ProcessingInstructionNode\n  | TagOpenNode\n  | TagCloseNode\n\nexport interface Saxy {\n  on<U extends SaxyEventNames>(event: U, listener: SaxyEvents[U]): this\n  on(event: string | symbol | Event, listener: (...args: any[]) => void): this\n  once<U extends SaxyEventNames>(event: U, listener: SaxyEvents[U]): this\n}\n\n/**\n * Schema for defining allowed tags and their children\n */\nexport type TagSchema = {\n  [topLevelTag: string]: string[] // Allowed child tags\n}\n\n/**\n * Nodes that can be found inside an XML stream.\n */\nconst Node = {\n  text: 'text',\n  cdata: 'cdata',\n  comment: 'comment',\n  processingInstruction: 'processinginstruction',\n  tagOpen: 'tagopen',\n  tagClose: 'tagclose',\n  // markupDeclaration: 'markupDeclaration',\n} as Record<string, SaxyEventNames>\n\n/**\n * Expand a piece of XML text by replacing all XML entities by\n * their canonical value. Ignore invalid and unknown entities.\n *\n * @param input A string of XML text\n * @return The input string, expanded\n */\nconst parseEntities = (input: string) => {\n  let position = 0,\n    next = 0\n  const parts = []\n\n  while ((next = input.indexOf('&', position)) !== -1) {\n    // remember anything there was before the entity\n    if (next > position) {\n      parts.push(input.slice(position, next))\n    }\n\n    const end = input.indexOf(';', next)\n\n    // ignore unterminated entities\n    if (end === -1) {\n      parts.push(input.slice(next))\n      break\n    }\n\n    const entity = input.slice(next + 1, end)\n\n    if (entity === 'quot') {\n      parts.push('\"')\n    } else if (entity === 'amp') {\n      parts.push('&')\n    } else if (entity === 'apos') {\n      parts.push(\"'\")\n    } else if (entity === 'lt') {\n      parts.push('<')\n    } else if (entity === 'gt') {\n      parts.push('>')\n    } else {\n      // ignore unrecognized character entities\n      if (entity[0] !== '#') {\n        parts.push('&' + entity + ';')\n      } else {\n        // hexadecimal numeric entities\n        if (entity[1] === 'x') {\n          const value = parseInt(entity.slice(2), 16)\n\n          // ignore non-numeric numeric entities\n          if (isNaN(value)) {\n            parts.push('&' + entity + ';')\n          } else {\n            parts.push(String.fromCharCode(value))\n          }\n        } else {\n          // decimal numeric entities\n          const value = parseInt(entity.slice(1), 10)\n\n          // ignore non-numeric numeric entities\n          if (isNaN(value)) {\n            parts.push('&' + entity + ';')\n          } else {\n            parts.push(String.fromCharCode(value))\n          }\n        }\n      }\n    }\n\n    position = end + 1\n  }\n\n  if (position < input.length) {\n    parts.push(input.slice(position))\n  }\n\n  return parts.join('')\n}\n\n/**\n * Parse a string of XML attributes to a map of attribute names to their values.\n *\n * @param input A string of XML attributes\n * @throws If the string is malformed\n * @return A map of attribute names to their values\n */\nexport const parseAttrs = (input: string) => {\n  const attrs = {} as Record<string, unknown>\n  const end = input.length\n  let position = 0\n\n  while (position < end) {\n    // Skip all whitespace\n    if (isWhitespace(input[position])) {\n      position += 1\n      continue\n    }\n\n    // Check that the attribute name contains valid chars\n    const startName = position\n\n    while (input[position] !== '=' && position < end) {\n      if (isWhitespace(input[position])) {\n        throw new Error('Attribute names may not contain whitespace')\n      }\n\n      position += 1\n    }\n\n    // This is XML, so we need a value for the attribute\n    if (position === end) {\n      throw new Error('Expected a value for the attribute')\n    }\n\n    const attrName = input.slice(startName, position)\n    position += 1\n    const startQuote = input[position]\n    position += 1\n\n    if (startQuote !== '\"' && startQuote !== \"'\") {\n      throw new Error('Attribute values should be quoted')\n    }\n\n    const endQuote = input.indexOf(startQuote, position)\n\n    if (endQuote === -1) {\n      throw new Error('Unclosed attribute value')\n    }\n\n    const attrValue = input.slice(position, endQuote)\n\n    attrs[attrName] = attrValue\n    position = endQuote + 1\n  }\n\n  return attrs\n}\n\n/**\n * Find the first character in a string that matches a predicate\n * while being outside the given delimiters.\n *\n * @param haystack String to search in\n * @param predicate Checks whether a character is permissible\n * @param [delim=''] Delimiter inside which no match should be\n * returned. If empty, all characters are considered.\n * @param [fromIndex=0] Start the search from this index\n * @return Index of the first match, or -1 if no match\n */\nconst findIndexOutside = (\n  haystack: string,\n  predicate: Function,\n  delim = '',\n  fromIndex = 0\n) => {\n  const length = haystack.length\n  let index = fromIndex\n  let inDelim = false\n\n  while (index < length && (inDelim || !predicate(haystack[index]))) {\n    if (haystack[index] === delim) {\n      inDelim = !inDelim\n    }\n\n    ++index\n  }\n\n  return index === length ? -1 : index\n}\n\n/**\n * Parse an XML stream and emit events corresponding\n * to the different tokens encountered.\n */\nexport class Saxy extends Transform {\n  private _decoder: StringDecoder\n  private _tagStack: string[]\n  private _waiting: { token: string; data: unknown } | null\n  private _schema: TagSchema | null\n  private _pendingEntity: string | null\n\n  /**\n   * Parse a string of XML attributes to a map of attribute names\n   * to their values\n   *\n   * @param input A string of XML attributes\n   * @throws If the string is malformed\n   * @return A map of attribute names to their values\n   */\n  static parseAttrs = parseAttrs\n\n  /**\n   * Expand a piece of XML text by replacing all XML entities\n   * by their canonical value. Ignore invalid and unknown\n   * entities\n   *\n   * @param input A string of XML text\n   * @return The input string, expanded\n   */\n  static parseEntities = parseEntities\n\n  /**\n   * Create a new parser instance.\n   * @param schema Optional schema defining allowed top-level tags and their children\n   */\n  constructor(schema?: TagSchema) {\n    super({ decodeStrings: false })\n\n    // String decoder instance\n    const state = this._writableState\n    this._decoder = new StringDecoder(state.defaultEncoding)\n\n    // Stack of tags that were opened up until the current cursor position\n    this._tagStack = []\n\n    // Not waiting initially\n    this._waiting = null\n\n    // Store schema if provided\n    this._schema = schema || null\n\n    // Pending entity for incomplete entities\n    this._pendingEntity = null\n  }\n\n  /**\n   * Handle a chunk of data written into the stream.\n   *\n   * @param chunk Chunk of data.\n   * @param encoding Encoding of the string, or 'buffer'.\n   * @param callback Called when the chunk has been parsed, with\n   * an optional error argument.\n   */\n  public _write(\n    chunk: Buffer | string,\n    encoding: string,\n    callback: NextFunction\n  ) {\n    const data =\n      encoding === 'buffer'\n        ? this._decoder.write(chunk as Buffer)\n        : (chunk as string)\n\n    this._parseChunk(data, callback)\n  }\n\n  /**\n   * Handle the end of incoming data.\n   *\n   * @param callback\n   */\n  public _final(callback: NextFunction) {\n    // Make sure all data has been extracted from the decoder\n    this._parseChunk(this._decoder.end(), (err?: Error) => {\n      if (err) {\n        callback(err)\n        return\n      }\n\n      // Handle any remaining pending entity\n      if (this._pendingEntity) {\n        this.emit(Node.text, { contents: this._pendingEntity })\n      }\n\n      // Handle unclosed nodes\n      if (this._waiting !== null) {\n        switch (this._waiting.token) {\n          case Node.text:\n            // Text nodes are implicitly closed\n            this.emit('text', { contents: this._waiting.data })\n            break\n          case Node.cdata:\n            callback(new Error('Unclosed CDATA section'))\n            return\n          case Node.comment:\n            callback(new Error('Unclosed comment'))\n            return\n          case Node.processingInstruction:\n            callback(new Error('Unclosed processing instruction'))\n            return\n          case Node.tagOpen:\n          case Node.tagClose:\n            // We do not distinguish between unclosed opening\n            // or unclosed closing tags\n            // callback(new Error('Unclosed tag'))\n            return\n          default:\n          // Pass\n        }\n      }\n\n      if (this._tagStack.length !== 0) {\n        // callback(new Error(`Unclosed tags: ${this._tagStack.join(',')}`))\n        return\n      }\n\n      callback()\n    })\n  }\n\n  /**\n   * Immediately parse a complete chunk of XML and close the stream.\n   *\n   * @param input Input chunk.\n   */\n  public parse(input: Buffer | string): this {\n    this.end(input)\n    return this\n  }\n\n  /**\n   * Put the stream into waiting mode, which means we need more data\n   * to finish parsing the current token.\n   *\n   * @param token Type of token that is being parsed.\n   * @param data Pending data.\n   */\n  private _wait(token: string, data: unknown) {\n    this._waiting = { token, data }\n  }\n\n  /**\n   * Put the stream out of waiting mode.\n   *\n   * @return Any data that was pending.\n   */\n  private _unwait() {\n    if (this._waiting === null) {\n      return ''\n    }\n\n    const data = this._waiting.data\n    this._waiting = null\n    return data\n  }\n\n  /**\n   * Handle the opening of a tag in the text stream.\n   *\n   * Push the tag into the opened tag stack and emit the\n   * corresponding event on the event emitter.\n   *\n   * @param node Information about the opened tag.\n   * @param rawTag The raw tag text including angle brackets\n   */\n  private _handleTagOpening(node: TagOpenNode, rawTag: string) {\n    const { name } = node\n\n    // If we have a schema, validate against it\n    if (this._schema) {\n      // For top-level tags\n      if (this._tagStack.length === 0) {\n        // Convert to text if not in schema\n        if (!this._schema[name]) {\n          this.emit(Node.text, { contents: rawTag })\n          return\n        }\n      }\n      // For nested tags\n      else {\n        const parentTag = this._tagStack[this._tagStack.length - 1]\n        // Convert to text if parent not in schema or this tag not allowed as child\n        if (\n          !this._schema[parentTag] ||\n          !this._schema[parentTag].includes(name)\n        ) {\n          this.emit(Node.text, { contents: rawTag })\n          return\n        }\n      }\n    }\n\n    if (!node.isSelfClosing) {\n      this._tagStack.push(node.name)\n    }\n\n    this.emit(Node.tagOpen, node)\n  }\n\n  /**\n   * Parse a XML chunk.\n   *\n   * @private\n   * @param input A string with the chunk data.\n   * @param callback Called when the chunk has been parsed, with\n   * an optional error argument.\n   */\n  private _parseChunk(input: string, callback: NextFunction) {\n    // Handle pending entity if exists\n    if (this._pendingEntity) {\n      input = this._pendingEntity + input\n      this._pendingEntity = null\n    }\n\n    // Use pending data if applicable and get out of waiting mode\n    const waitingData = this._unwait()\n    input = waitingData + input\n\n    let chunkPos = 0\n    const end = input.length\n\n    while (chunkPos < end) {\n      if (\n        input[chunkPos] !== '<' ||\n        (chunkPos + 1 < end && !this._isXMLTagStart(input, chunkPos + 1))\n      ) {\n        // Find next potential tag, but verify it's actually a tag\n        let nextTag = input.indexOf('<', chunkPos)\n        while (\n          nextTag !== -1 &&\n          nextTag + 1 < end &&\n          !this._isXMLTagStart(input, nextTag + 1)\n        ) {\n          nextTag = input.indexOf('<', nextTag + 1)\n        }\n\n        // We read a TEXT node but there might be some\n        // more text data left, so we wait\n        if (nextTag === -1) {\n          let chunk = input.slice(chunkPos)\n\n          // Check for incomplete entity at end\n          const lastAmp = chunk.lastIndexOf('&')\n          if (lastAmp !== -1 && chunk.indexOf(';', lastAmp) === -1) {\n            // Store incomplete entity for next chunk\n            this._pendingEntity = chunk.slice(lastAmp)\n            chunk = chunk.slice(0, lastAmp)\n          }\n\n          if (chunk.length > 0) {\n            chunk = parseEntities(chunk)\n            this.emit(Node.text, { contents: chunk })\n          }\n\n          chunkPos = end\n          break\n        }\n\n        // A tag follows, so we can be confident that\n        // we have all the data needed for the TEXT node\n        let chunk = input.slice(chunkPos, nextTag)\n\n        // Check for incomplete entity at end\n        const lastAmp = chunk.lastIndexOf('&')\n        if (lastAmp !== -1 && chunk.indexOf(';', lastAmp) === -1) {\n          // Store incomplete entity for next chunk\n          this._pendingEntity = chunk.slice(lastAmp)\n          chunk = chunk.slice(0, lastAmp)\n        }\n\n        // Only emit non-whitespace text or text within a single tag (not between tags)\n        if (chunk.length > 0) {\n          chunk = parseEntities(chunk)\n          this.emit(Node.text, { contents: chunk })\n        }\n\n        chunkPos = nextTag\n      }\n\n      // Invariant: the cursor now points on the name of a tag,\n      // after an opening angled bracket\n      chunkPos += 1\n\n      // Recognize regular tags (< ... >)\n      const tagClose = findIndexOutside(\n        input,\n        (char: string) => char === '>',\n        '\"',\n        chunkPos\n      )\n\n      if (tagClose === -1) {\n        this._wait(Node.tagOpen, input.slice(chunkPos - 1))\n        break\n      }\n\n      // Check if the tag is a closing tag\n      if (input[chunkPos] === '/') {\n        const tagName = input.slice(chunkPos + 1, tagClose)\n        const stackedTagName = this._tagStack[this._tagStack.length - 1]\n\n        // Convert closing tag to text if it doesn't match schema validation\n        if (this._schema) {\n          // For top-level tags\n          if (this._tagStack.length === 1) {\n            if (!this._schema[tagName]) {\n              const rawTag = input.slice(chunkPos - 1, tagClose + 1)\n              this.emit(Node.text, { contents: rawTag })\n              chunkPos = tagClose + 1\n              continue\n            }\n          }\n          // For nested tags\n          else {\n            const parentTag = this._tagStack[this._tagStack.length - 2]\n            if (\n              !this._schema[parentTag] ||\n              !this._schema[parentTag].includes(tagName)\n            ) {\n              const rawTag = input.slice(chunkPos - 1, tagClose + 1)\n              this.emit(Node.text, { contents: rawTag })\n              chunkPos = tagClose + 1\n              continue\n            }\n          }\n        }\n\n        this._tagStack.pop()\n\n        // Only emit if the tag matches what we expect\n        if (stackedTagName === tagName) {\n          this.emit(Node.tagClose, { name: tagName })\n        }\n\n        chunkPos = tagClose + 1\n        continue\n      }\n\n      // Check if the tag is self-closing\n      const isSelfClosing = input[tagClose - 1] === '/'\n      let realTagClose = isSelfClosing ? tagClose - 1 : tagClose\n\n      // Extract the tag name and attributes\n      const whitespace = input.slice(chunkPos).search(/\\s/)\n\n      // Get the raw tag text for potential text node conversion\n      const rawTag = input.slice(chunkPos - 1, tagClose + 1)\n\n      if (whitespace === -1 || whitespace >= tagClose - chunkPos) {\n        // Tag without any attribute\n        this._handleTagOpening(\n          {\n            name: input.slice(chunkPos, realTagClose),\n            attrs: '',\n            isSelfClosing,\n          },\n          rawTag\n        )\n      } else if (whitespace === 0) {\n        // Invalid tag starting with whitespace - emit as text\n        this.emit(Node.text, { contents: rawTag })\n      } else {\n        // Tag with attributes\n        this._handleTagOpening(\n          {\n            name: input.slice(chunkPos, chunkPos + whitespace),\n            attrs: input.slice(chunkPos + whitespace, realTagClose),\n            isSelfClosing,\n          },\n          rawTag\n        )\n      }\n\n      chunkPos = tagClose + 1\n    }\n\n    callback()\n  }\n\n  /**\n   * Check if a potential XML tag start is actually a valid tag\n   * @param input The input string\n   * @param pos Position after the < character\n   * @returns true if this is a valid XML tag start\n   */\n  private _isXMLTagStart(input: string, pos: number): boolean {\n    // Valid XML tags must start with a letter, underscore or colon\n    // https://www.w3.org/TR/xml/#NT-NameStartChar\n    const firstChar = input[pos]\n    return /[A-Za-z_:]/.test(firstChar) || firstChar === '/'\n  }\n}\n",
            "tokens": 5968
        },
        "common/src/util/string.knowledge.md": {
            "content": "# String Utilities\n\n## Implementation Guidelines\n\n### Pluralization\n- Consider all cases when implementing word transformations:\n  - Zero quantity may need special handling\n  - Negative numbers\n  - Decimal numbers\n  - Language/locale specific rules\n  - Irregular plurals (e.g., child -> children)\n  - Words ending in y: only change to 'ies' if preceded by a consonant (e.g., \"fly\" -> \"flies\" but \"day\" -> \"days\", \"key\" -> \"keys\")\n  - Words ending in s, ch, sh, x: add 'es'\n  - Special suffixes (-es, -ies)\n\nSimple implementations can lead to bugs. Prefer using established i18n/l10n libraries for production text transformations.\n\n### General String Transformation Guidelines\n- Avoid quick, simple implementations for language-specific transformations\n- Consider edge cases before implementing string manipulation functions\n- Document assumptions and limitations in comments\n- For text displayed to users, use i18n libraries rather than custom implementations\n- Test with a variety of inputs including:\n  - Empty strings\n  - Special characters\n  - Unicode/emoji\n  - Very long strings\n  - Different locales\n\n### JSON in Strings Pattern\nWhen modifying JSON content within strings:\n- Use regex to extract the specific JSON portion first\n- Parse the extracted content to work with it as an object\n- Make modifications to the parsed object\n- Stringify the modified content\n- Use regex replacement to put it back in the original string\n- Always include fallback behavior if parsing fails\n- Extract transformation logic into a reusable helper function:\n  ```\n\n### Message Content Pattern\nWhen transforming message content:\n- Filter out unwanted content types before transformation\n- Use ts-pattern's match for type-safe content handling\n- Avoid producing null values that would need filtering\n- Example pattern:\n  ```typescript\n  match(message)\n    .with({ content: P.array() }, (msg) => ({\n      ...msg,\n      content: msg.content.reduce<typeof msg.content>(\n        (acc, contentObj) => [\n          ...acc,\n          ...match(contentObj)\n            .with({ type: 'unwanted-type' }, () => [])\n            .with({ type: 'specific-type', content: P.string }, (obj) => [{\n              ...obj,\n              content: transform(obj.content)\n            }])\n            .with({ type: 'text', text: P.string }, (obj) => [{\n              ...obj,\n              text: transform(obj.text)\n            }])\n            .otherwise((obj) => [obj])\n        ],\n        []\n      )\n    }))\n    .with({ content: P.string }, handleString)\n    .otherwise(msg => msg)\n  ```\n\nThe pattern above combines filtering and transformation:\n- Uses reduce to handle filtering and transformation in a single pass\n- Each match case returns an array (even for filtering - returns empty array)\n- Spreads match results into accumulator for consistent array handling\n- Avoids intermediate arrays from map+filter chain\n- More declarative and efficient than separate operations\n- Keeps all type handling in one placetypescript\n  const transformJsonInString = <T = unknown>(\n    content: string,\n    field: string,\n    transform: (json: T) => unknown,\n    fallback: string\n  ): string => {\n    const pattern = new RegExp(`\"${field}\"\\\\s*:\\\\s*(\\\\[[^\\\\]]*\\\\]|\\\\{[^}]*\\\\})`)\n    const match = content.match(pattern)\n    if (!match) return content\n\n    try {\n      const json = JSON.parse(match[1])\n      const transformed = transform(json)\n      return content.replace(\n        new RegExp(`\"${field}\"\\\\s*:\\\\s*\\\\[[^\\\\]]*\\\\]|\\\\{[^}]*\\\\}`, 'g'),\n        `\"${field}\":${JSON.stringify(transformed)}`\n      )\n    } catch {\n      return content.replace(\n        new RegExp(`\"${field}\"\\\\s*:\\\\s*\\\\[[^\\\\]]*\\\\]|\\\\{[^}]*\\\\}`, 'g'),\n        `\"${field}\":${fallback}`\n      )\n    }\n  }\n  ```\n- This pattern supports both arrays and objects\n- Provides clean error handling with fallbacks\n- Makes transformations declarative and reusable\n- Use generic type parameter to ensure type safety:\n  ```typescript\n  // Example with typed array\n  transformJsonInString<Array<{ source?: string }>>(\n    content,\n    'logs',\n    (logs) => logs.filter(log => log?.source === 'tool'),\n    '[]'\n  )\n  ```\n",
            "tokens": 1283
        },
        "common/src/util/string.ts": {
            "content": "import { sumBy } from 'lodash'\n\nexport const truncateString = (str: string, maxLength: number) => {\n  if (str.length <= maxLength) {\n    return str\n  }\n  return str.slice(0, maxLength) + '...'\n}\n\nexport const truncateStringWithMessage = ({\n  str,\n  maxLength,\n  message = 'TRUNCATED_DUE_TO_LENGTH',\n  remove = 'END',\n}: {\n  str: string\n  maxLength: number\n  message?: string\n  remove?: 'END' | 'START' | 'MIDDLE'\n}) => {\n  if (str.length <= maxLength) {\n    return str\n  }\n\n  if (remove === 'END') {\n    const suffix = `\\n[...${message}]`\n    return str.slice(0, maxLength - suffix.length) + suffix\n  }\n  if (remove === 'START') {\n    const prefix = `[${message}...]\\n`\n    return prefix + str.slice(0, maxLength - prefix.length)\n  }\n\n  const middle = `\\n[...${message}...]\\n`\n  const length = Math.floor((maxLength - middle.length) / 2)\n  return str.slice(0, length) + middle + str.slice(-length)\n}\n\n/**\n * Check if a character is a whitespace character according\n * to the XML spec (space, carriage return, line feed or tab)\n *\n * @param character Character to check\n * @return Whether the character is whitespace or not\n */\nexport const isWhitespace = (character: string) => /\\s/.test(character)\n\nexport const replaceNonStandardPlaceholderComments = (\n  content: string,\n  replacement: string\n): string => {\n  const commentPatterns = [\n    // JSX comments (match this first)\n    {\n      regex:\n        /{\\s*\\/\\*\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\s*\\.{3})?\\s*\\*\\/\\s*}/gi,\n      placeholder: replacement,\n    },\n    // C-style comments (C, C++, Java, JavaScript, TypeScript, etc.)\n    {\n      regex:\n        /\\/\\/\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\s*\\.{3})?/gi,\n      placeholder: replacement,\n    },\n    {\n      regex:\n        /\\/\\*\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\s*\\.{3})?\\s*\\*\\//gi,\n      placeholder: replacement,\n    },\n    // Python, Ruby, R comments\n    {\n      regex:\n        /#\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\s*\\.{3})?/gi,\n      placeholder: replacement,\n    },\n    // HTML-style comments\n    {\n      regex:\n        /<!--\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\s*\\.{3})?\\s*-->/gi,\n      placeholder: replacement,\n    },\n    // SQL, Haskell, Lua comments\n    {\n      regex:\n        /--\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\s*\\.{3})?/gi,\n      placeholder: replacement,\n    },\n    // MATLAB comments\n    {\n      regex:\n        /%\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\s*\\.{3})?/gi,\n      placeholder: replacement,\n    },\n  ]\n\n  let updatedContent = content\n\n  for (const { regex, placeholder } of commentPatterns) {\n    updatedContent = updatedContent.replaceAll(regex, placeholder)\n  }\n\n  return updatedContent\n}\n\nexport const randBoolFromStr = (str: string) => {\n  return sumBy(str.split(''), (char) => char.charCodeAt(0)) % 2 === 0\n}\n\nexport const pluralize = (count: number, word: string) => {\n  if (count === 1) return `${count} ${word}`\n\n  // Handle words ending in 'y' (unless preceded by a vowel)\n  if (word.endsWith('y') && !word.match(/[aeiou]y$/)) {\n    return `${count} ${word.slice(0, -1) + 'ies'}`\n  }\n\n  // Handle words ending in s, sh, ch, x, z, o\n  if (word.match(/[sxz]$/) || word.match(/[cs]h$/) || word.match(/o$/)) {\n    return `${count} ${word + 'es'}`\n  }\n\n  // Handle words ending in f/fe\n  if (word.endsWith('f')) {\n    return `${count} ${word.slice(0, -1) + 'ves'}`\n  }\n  if (word.endsWith('fe')) {\n    return `${count} ${word.slice(0, -2) + 'ves'}`\n  }\n\n  return `${count} ${word + 's'}`\n}\n\n/**\n * Safely replaces all occurrences of a search string with a replacement string,\n * escaping special replacement patterns (like $) in the replacement string.\n */\nexport const capitalize = (str: string): string => {\n  if (!str) return str\n  return str.charAt(0).toUpperCase() + str.slice(1).toLowerCase()\n}\n\n/**\n * Converts a snake_case string to Title Case\n * Example: \"add_subgoal\" -> \"Add Subgoal\"\n */\nexport const snakeToTitleCase = (str: string): string => {\n  return str\n    .split('_')\n    .map((word) => capitalize(word))\n    .join(' ')\n}\n\n/**\n * Ensures a URL has the appropriate protocol (http:// or https://)\n * Uses http:// for localhost and local IPs, https:// for all other domains\n */\nexport const ensureUrlProtocol = (url: string): string => {\n  if (url.startsWith('http://') || url.startsWith('https://')) {\n    return url\n  }\n\n  if (url.startsWith('localhost') || url.match(/^127\\.\\d+\\.\\d+\\.\\d+/)) {\n    return `http://${url}`\n  }\n\n  return `https://${url}`\n}\n\nexport const safeReplace = (\n  content: string,\n  searchStr: string,\n  replaceStr: string\n): string => {\n  const escapedReplaceStr = replaceStr.replace(/\\$/g, '$$$$')\n  return content.replace(searchStr, escapedReplaceStr)\n}\n\nexport const hasLazyEdit = (content: string) => {\n  const cleanedContent = content.toLowerCase().trim()\n  return (\n    cleanedContent.includes('... existing code ...') ||\n    cleanedContent.includes('// rest of the') ||\n    cleanedContent.includes('# rest of the') ||\n    // Match various comment styles with ellipsis and specific words\n    /\\/\\/\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\.{3})?/.test(\n      cleanedContent\n    ) || // C-style single line\n    /\\/\\*\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\.{3})?\\s*\\*\\//.test(\n      cleanedContent\n    ) || // C-style multi-line\n    /#\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\.{3})?/.test(\n      cleanedContent\n    ) || // Python/Ruby style\n    /<!--\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\.{3})?\\s*-->/.test(\n      cleanedContent\n    ) || // HTML style\n    /--\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\.{3})?/.test(\n      cleanedContent\n    ) || // SQL/Haskell style\n    /%\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\.{3})?/.test(\n      cleanedContent\n    ) || // MATLAB style\n    /{\\s*\\/\\*\\s*\\.{3}.*(?:rest|unchanged|keep|file|existing|some).*(?:\\.{3})?\\s*\\*\\/\\s*}/.test(\n      cleanedContent\n    ) // JSX style\n  )\n}\n\n/**\n * Extracts a JSON field from a string, transforms it, and puts it back.\n * Handles both array and object JSON values.\n * @param content The string containing JSON-like content\n * @param field The field name to find and transform\n * @param transform Function to transform the parsed JSON value\n * @param fallback String to use if parsing fails\n * @returns The content string with the transformed JSON field\n */\nexport const transformJsonInString = <T = unknown>(\n  content: string,\n  field: string,\n  transform: (json: T) => unknown,\n  fallback: string\n): string => {\n  // Use a non-greedy match for objects/arrays to prevent over-matching\n  const pattern = new RegExp(`\"${field}\"\\\\s*:\\\\s*(\\\\{[^}]*?\\\\}|\\\\[[^\\\\]]*?\\\\])`)\n  const match = content.match(pattern)\n\n  if (!match) {\n    return content\n  }\n\n  try {\n    const json = JSON.parse(match[1])\n    const transformed = transform(json)\n\n    // Important: Only replace the exact matched portion to prevent duplicates\n    return content.replace(\n      match[0],\n      `\"${field}\":${JSON.stringify(transformed)}`\n    )\n  } catch (error) {\n    // Only replace the exact matched portion even in error case\n    return content.replace(match[0], `\"${field}\":${fallback}`)\n  }\n}\n\n/**\n * Generates a compact unique identifier by combining timestamp bits with random bits.\n * Takes last 24 bits of timestamp (enough for months) and 8 random bits.\n * Encodes in base36 for very compact strings (~5-6 chars).\n * @param prefix Optional prefix to add to the ID\n * @returns A unique string ID\n * @example\n * generateCompactId()      // => \"1a2b3c\"\n * generateCompactId('msg-') // => \"msg-1a2b3c\"\n */\nexport const generateCompactId = (prefix?: string): string => {\n  const timestamp = Date.now() & 0xffffff // Last 24 bits of timestamp\n  const random = Math.floor(Math.random() * 0xff) // 8 random bits\n  const str = ((timestamp << 8) | random).toString(36).replace(/^-/, '') // Remove leading dash if present\n  return prefix ? `${prefix}${str}` : str\n}\n\n/**\n * Removes null characters from a string\n */\nexport const stripNullChars = (str: string): string => {\n  return str.replace(/\\u0000/g, '')\n}\n\nconst ansiRegex = /\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])/g\nexport function stripColors(str: string): string {\n  return str.replace(ansiRegex, '')\n}\n",
            "tokens": 3294
        },
        "common/src/websockets/websocket-client.ts": {
            "content": "import { WebSocket } from 'ws'\n\nimport { ClientAction, ServerAction } from '../actions'\nimport {\n  ClientMessage,\n  ClientMessageType,\n  ServerMessage,\n} from './websocket-schema'\n\n// mqp: useful for debugging\nconst VERBOSE_LOGGING = false\n\nconst TIMEOUT_MS = 120_000\n\nconst RECONNECT_WAIT_MS = 5_000\n\ntype ConnectingState = typeof WebSocket.CONNECTING\ntype OpenState = typeof WebSocket.OPEN\ntype ClosingState = typeof WebSocket.CLOSING\ntype ClosedState = typeof WebSocket.CLOSED\n\nexport type ReadyState =\n  | OpenState\n  | ConnectingState\n  | ClosedState\n  | ClosingState\n\nexport function formatState(state: ReadyState) {\n  switch (state) {\n    case WebSocket.CONNECTING:\n      return 'connecting'\n    case WebSocket.OPEN:\n      return 'open'\n    case WebSocket.CLOSING:\n      return 'closing'\n    case WebSocket.CLOSED:\n      return 'closed'\n    default:\n      throw new Error('Invalid websocket state.')\n  }\n}\n\ntype OutstandingTxn = {\n  resolve: () => void\n  reject: (err: Error) => void\n  timeout?: any\n}\n\n/** Client for the API websocket realtime server. Automatically manages reconnection\n * and resubscription on disconnect, and allows subscribers to get a callback\n * when something is broadcasted. */\nexport class APIRealtimeClient {\n  ws!: WebSocket\n  url: string\n  // Callbacks subscribed to individual actions.\n  subscribers: Map<ServerAction['type'], ((action: ServerAction) => void)[]>\n  txid: number\n  // all txns that are in flight, with no ack/error/timeout\n  txns: Map<number, OutstandingTxn>\n  connectTimeout?: any\n  heartbeat?: any\n  hadError = false\n  onError: () => void\n  onReconnect: () => void\n\n  constructor(url: string, onError: () => void, onReconnect: () => void) {\n    this.url = url\n    this.txid = 0\n    this.txns = new Map()\n    this.subscribers = new Map()\n    this.onError = onError\n    this.onReconnect = onReconnect\n  }\n\n  get state() {\n    return this.ws.readyState as ReadyState\n  }\n\n  close() {\n    this.ws.close(1000, 'Closed manually.')\n    clearTimeout(this.connectTimeout)\n  }\n\n  connect() {\n    // you may wish to refer to https://websockets.spec.whatwg.org/\n    // in order to check the semantics of events etc.\n    this.ws = new WebSocket(this.url)\n    this.ws.onmessage = (ev) => {\n      if (this.hadError) {\n        this.hadError = false\n        this.onReconnect()\n      }\n      this.receiveMessage(JSON.parse(ev.data as any))\n    }\n    this.ws.onerror = (ev) => {\n      if (!this.hadError) {\n        this.onError()\n        this.hadError = true\n      }\n      // this can fire without an onclose if this is the first time we ever try\n      // to connect, so we need to turn on our reconnect in that case\n      this.waitAndReconnect()\n    }\n    this.ws.onclose = (ev) => {\n      // note that if the connection closes due to an error, onerror fires and then this\n      if (VERBOSE_LOGGING) {\n        console.info(`API websocket closed with code=${ev.code}: ${ev.reason}`)\n      }\n      clearInterval(this.heartbeat)\n\n      // mqp: we might need to change how the txn stuff works if we ever want to\n      // implement \"wait until i am subscribed, and then do something\" in a component.\n      // right now it cannot be reliably used to detect that in the presence of reconnects\n      for (const txn of Array.from(this.txns.values())) {\n        clearTimeout(txn.timeout)\n        // NOTE (James): Don't throw an error when the websocket is closed...\n        // This seems to be happening, but the client can recover.\n        txn.resolve()\n        // txn.reject(new Error('Websocket was closed.'))\n      }\n      this.txns.clear()\n\n      // 1000 is RFC code for normal on-purpose closure\n      if (ev.code !== 1000) {\n        this.waitAndReconnect()\n      }\n    }\n    return new Promise<void>((resolve) => {\n      this.ws.onopen = (_ev) => {\n        if (VERBOSE_LOGGING) {\n          console.info('API websocket opened.')\n        }\n        this.heartbeat = setInterval(\n          async () => this.sendMessage('ping', {}).catch(() => {}),\n          30000\n        )\n\n        resolve()\n      }\n    })\n  }\n\n  waitAndReconnect() {\n    if (this.connectTimeout == null) {\n      this.connectTimeout = setTimeout(() => {\n        this.connectTimeout = undefined\n        this.connect()\n      }, RECONNECT_WAIT_MS)\n    }\n  }\n\n  forceReconnect() {\n    // Close the current connection if it's open\n    if (this.ws && this.state !== WebSocket.CLOSED) {\n      this.ws.close(1000, 'Forced reconnection due to server shutdown notice')\n    }\n\n    // Immediately attempt to reconnect\n    this.connect().catch((err) => {\n      console.error('Failed to reconnect after server shutdown notice:', err)\n      // Still set up delayed reconnect as fallback\n      this.waitAndReconnect()\n    })\n  }\n\n  receiveMessage(msg: ServerMessage) {\n    if (VERBOSE_LOGGING) {\n      console.info('< Incoming API websocket message: ', msg)\n    }\n    switch (msg.type) {\n      case 'action': {\n        const action = msg.data\n        const subscribers = this.subscribers.get(action.type) ?? []\n        for (const callback of subscribers) {\n          callback(action)\n        }\n        return\n      }\n      case 'ack': {\n        if (msg.txid != null) {\n          const txn = this.txns.get(msg.txid)\n          if (txn == null) {\n            // mqp: only reason this should happen is getting an ack after timeout\n            console.warn(`Websocket message with old txid=${msg.txid}.`)\n          } else {\n            clearTimeout(txn.timeout)\n            if (msg.error != null) {\n              txn.reject(new Error(msg.error))\n            } else {\n              txn.resolve()\n            }\n            this.txns.delete(msg.txid)\n          }\n        }\n        return\n      }\n      default:\n        console.warn(`Unknown API websocket message type received: ${msg}`)\n    }\n  }\n\n  async sendMessage<T extends ClientMessageType>(\n    type: T,\n    data: Omit<ClientMessage<T>, 'type' | 'txid'>\n  ) {\n    if (VERBOSE_LOGGING) {\n      console.info(`> Outgoing API websocket ${type} message: `, data)\n    }\n    if (this.state === WebSocket.OPEN) {\n      return new Promise<void>((resolve, reject) => {\n        const txid = this.txid++\n        const timeout = setTimeout(() => {\n          this.txns.delete(txid)\n          reject(new Error(`Websocket message with txid ${txid} timed out.`))\n        }, TIMEOUT_MS)\n        this.txns.set(txid, { resolve, reject, timeout })\n        this.ws.send(JSON.stringify({ type, txid, ...data }))\n      })\n    } else {\n      // expected if components in the code try to subscribe or unsubscribe\n      // while the socket is closed -- in this case we expect to get the state\n      // fixed up in the websocket onopen handler when we reconnect\n    }\n  }\n\n  async sendAction(action: ClientAction) {\n    try {\n      return await this.sendMessage('action', {\n        data: action,\n      })\n    } catch (e) {\n      // Print the error message for debugging.\n      console.error(\n        'Error sending action:',\n        action.type,\n        typeof e === 'object' && e !== null && 'message' in e ? e.message : e\n      )\n\n      console.log()\n      console.log('Codebuff is exiting due to an error.')\n      console.log('Make sure you are on the latest version of Codebuff!')\n      console.log('-----------------------------------')\n      console.log('Please run: npm install -g codebuff')\n      console.log('-----------------------------------')\n\n      process.exit(1)\n    }\n  }\n\n  subscribe<T extends ServerAction['type']>(\n    action: T,\n    callback: (action: Extract<ServerAction, { type: T }>) => void\n  ) {\n    const currSubscribers = this.subscribers.get(action) ?? []\n    this.subscribers.set(action, [\n      ...currSubscribers,\n      callback as (action: ServerAction) => void,\n    ])\n\n    return () => {\n      const newSubscribers = currSubscribers.filter((cb) => cb !== callback)\n      this.subscribers.set(action, newSubscribers)\n    }\n  }\n}\n",
            "tokens": 2533
        }
    },
    "fingerprint_id": "xCFSppJ96frbNwSQCCo2aJ1y6RocQvuGJbGQPKUgHrU-KrIAgzUQ",
    "user_input_id": "mc-input-kyohattc1y"
}